"""æ–‡æ¡£æœåŠ¡ï¼Œå¤„ç†æ–‡æ¡£ä¸Šä¼ ã€è§£æã€LightRAG é›†æˆ"""
import asyncio
import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from fastapi import UploadFile

import app.config as config
from app.services.conversation_service import ConversationService
from app.services.lightrag_service import LightRAGService
from app.services.mindmap_service import MindMapService
from app.storage.file_manager import FileManager
from app.utils.document_parser import DocumentParser

# å…¨å±€ä¿¡å·é‡ï¼šé™åˆ¶åŒæ—¶å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé¿å… LightRAG å¹¶å‘å†²çªï¼‰
_processing_semaphore = asyncio.Semaphore(1)  # åŒä¸€æ—¶é—´åªå¤„ç†1ä¸ªæ–‡æ¡£
# æ€ç»´è„‘å›¾ç”Ÿæˆä¿¡å·é‡ï¼šç¡®ä¿åŒä¸€å¯¹è¯çš„æ€ç»´è„‘å›¾ä¸²è¡Œç”Ÿæˆï¼ˆæ”¯æŒåˆå¹¶ï¼‰
_mindmap_semaphore: Dict[str, asyncio.Semaphore] = {}  # conversation_id -> Semaphore
_mindmap_lock = asyncio.Lock()  # ä¿æŠ¤ _mindmap_semaphore å­—å…¸çš„é”
# å¤„ç†é˜Ÿåˆ—é”ï¼šç¡®ä¿é˜Ÿåˆ—æ“ä½œçš„åŸå­æ€§
_queue_lock = asyncio.Lock()
# ç­‰å¾…é˜Ÿåˆ—ï¼š{ conversation_id: [document_id, ...] }
_processing_queue: Dict[str, List[str]] = {}


class DocumentService:
    """æ–‡æ¡£æœåŠ¡"""
    
    def __init__(self):
        self.conversation_service = ConversationService()
        self.lightrag_service = LightRAGService()
        self.mindmap_service = MindMapService()
        self.file_manager = FileManager()
        self.document_parser = DocumentParser()
        self.status_dir = Path(config.settings.conversations_metadata_dir) / "document_status"
        self.status_dir.mkdir(parents=True, exist_ok=True)
    
    def _clean_base64_and_save(self, text: str, conversation_id: str) -> Tuple[str, Dict[str, str]]:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„ base64 å­—ç¬¦ä¸²ï¼Œä¿å­˜åˆ° base_64.jsonï¼Œå¹¶è¿”å›æ¸…ç†åçš„æ–‡æœ¬å’Œæ˜ å°„å…³ç³»
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            conversation_id: å¯¹è¯ID
            
        Returns:
            (æ¸…ç†åçš„æ–‡æœ¬, base64æ˜ å°„å­—å…¸ {åºå·: base64å­—ç¬¦ä¸²})
        """
        # è·å– base64.json æ–‡ä»¶è·¯å¾„ï¼ˆä¸ kv_store_full_docs.json åŒçº§ï¼‰
        print("cleaning base64", "="*70)
        base_working_dir = Path(config.settings.lightrag_working_dir)
        base64_file = base_working_dir.parent / conversation_id / conversation_id / "base_64.json"
        base64_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½å·²æœ‰çš„ base64 æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        base64_map = {}
        if base64_file.exists():
            try:
                with open(base64_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    base64_map = existing_data if isinstance(existing_data, dict) else {}
            except:
                base64_map = {}
        
        # è·å–ä¸‹ä¸€ä¸ªåºå·ï¼ˆä»å·²æœ‰æœ€å¤§åºå·+1å¼€å§‹ï¼‰
        existing_indices = [int(k) for k in base64_map.keys() if k.isdigit()]
        next_index = max(existing_indices, default=0) + 1
        
        cleaned_text = text
        
        # 1. å¤„ç† <latexit> æ ‡ç­¾åŠå…¶å†…å®¹
        # åŒ¹é… <latexit> æ ‡ç­¾ï¼ŒåŒ…æ‹¬æ ‡ç­¾å±æ€§å’Œæ ‡ç­¾å†…å®¹
        latexit_pattern = r'<latexit[^>]*>([^<]*)</latexit>'
        
        def replace_latexit(match):
            nonlocal next_index
            full_match = match.group(0)
            tag_content = match.group(1).strip()
            
            # æå–æ ‡ç­¾ä¸­çš„ sha1_base64 å±æ€§å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            sha1_match = re.search(r'sha1_base64="([^"]+)"', full_match)
            
            # æå–æ ‡ç­¾å†…å®¹ä¸­çš„ base64 å­—ç¬¦ä¸²ï¼ˆé€šå¸¸æ˜¯é•¿å­—ç¬¦ä¸²ï¼‰
            base64_in_content = re.search(r'[A-Za-z0-9+/=]{50,}', tag_content)
            
            # ä¼˜å…ˆä½¿ç”¨æ ‡ç­¾å†…å®¹ä¸­çš„ base64ï¼Œå¦åˆ™ä½¿ç”¨ sha1_base64 å±æ€§
            if base64_in_content:
                base64_value = base64_in_content.group(0)
            elif sha1_match:
                base64_value = sha1_match.group(1)
            elif tag_content and len(tag_content) > 20:
                base64_value = tag_content
            else:
                return ""  # å¦‚æœå†…å®¹å¤ªçŸ­æˆ–æ²¡æœ‰ base64ï¼Œç›´æ¥ç§»é™¤
            
            index_str = str(next_index)
            base64_map[index_str] = base64_value
            next_index += 1
            return f"[BASE64_{index_str}]"
        
        cleaned_text = re.sub(latexit_pattern, replace_latexit, cleaned_text, flags=re.DOTALL)
        
        # 2. å¤„ç†ç‹¬ç«‹çš„ base64 å­—ç¬¦ä¸²ï¼ˆé•¿åº¦>=50ï¼Œä¸”ä¸åœ¨å·²æ›¿æ¢çš„å¼•ç”¨ä¸­ï¼‰
        # å…ˆæ ‡è®°å·²æ›¿æ¢çš„ä½ç½®ï¼Œé¿å…é‡å¤å¤„ç†
        standalone_base64_pattern = r'(?<!\[BASE64_)[A-Za-z0-9+/=]{50,}(?!\])'
        
        def replace_standalone(match):
            nonlocal next_index
            base64_str = match.group(0)
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ base64ï¼ˆä¸åŒ…å«ç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰
            if re.match(r'^[A-Za-z0-9+/=]+$', base64_str):
                index_str = str(next_index)
                base64_map[index_str] = base64_str
                next_index += 1
                return f"[BASE64_{index_str}]"
            return base64_str
        
        cleaned_text = re.sub(standalone_base64_pattern, replace_standalone, cleaned_text)
        
        # ä¿å­˜ base64 æ˜ å°„åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ–°å¢ï¼‰
        if base64_map:
            with open(base64_file, 'w', encoding='utf-8') as f:
                json.dump(base64_map, f, ensure_ascii=False, indent=2)
        
        return cleaned_text, base64_map
    
    def _get_status_file(self, conversation_id: str) -> Path:
        """è·å–çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
        return self.status_dir / f"{conversation_id}.json"
    
    def _load_status(self, conversation_id: str) -> Dict:
        """åŠ è½½æ–‡æ¡£çŠ¶æ€"""
        status_file = self._get_status_file(conversation_id)
        if status_file.exists():
            try:
                with open(status_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"documents": {}}
        return {"documents": {}}
    
    def _save_status(self, conversation_id: str, status: Dict):
        """ä¿å­˜æ–‡æ¡£çŠ¶æ€"""
        status_file = self._get_status_file(conversation_id)
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status, f, ensure_ascii=False, indent=2)
    
    def _get_subject_status_file(self, subject_id: str) -> Path:
        """è·å–çŸ¥è¯†åº“æ–‡æ¡£çŠ¶æ€æ–‡ä»¶è·¯å¾„"""
        subject_status_dir = Path(config.settings.conversations_metadata_dir) / "subjects" / subject_id
        subject_status_dir.mkdir(parents=True, exist_ok=True)
        return subject_status_dir / "documents.json"
    
    def _load_subject_status(self, subject_id: str) -> Dict:
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£çŠ¶æ€"""
        status_file = self._get_subject_status_file(subject_id)
        if status_file.exists():
            try:
                with open(status_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {"documents": {}}
        return {"documents": {}}
    
    def _save_subject_status(self, subject_id: str, status: Dict):
        """ä¿å­˜çŸ¥è¯†åº“æ–‡æ¡£çŠ¶æ€"""
        status_file = self._get_subject_status_file(subject_id)
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status, f, ensure_ascii=False, indent=2)
    
    def _validate_file(self, filename: str) -> tuple[bool, Optional[str]]:
        """éªŒè¯æ–‡ä»¶ç±»å‹
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        file_ext = Path(filename).suffix.lower().lstrip('.')
        if file_ext not in config.settings.allowed_extensions:
            return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œä»…æ”¯æŒ {', '.join(config.settings.allowed_extensions)}"
        
        return True, None
    
    async def _check_file_size(self, file_content: bytes) -> tuple[bool, Optional[str]]:
        """æ£€æŸ¥æ–‡ä»¶å¤§å°
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        file_size = len(file_content)
        
        if file_size > config.settings.max_file_size:
            return False, f"æ–‡ä»¶å¤§å° {file_size / 1024 / 1024:.2f}MB è¶…è¿‡é™åˆ¶ {config.settings.max_file_size / 1024 / 1024}MB"
        
        return True, None
    
    async def upload_documents(self, conversation_id: Optional[str], files: List[UploadFile]) -> Dict:
        """ä¸Šä¼ æ–‡æ¡£åˆ°å¯¹è¯
        
        Args:
            conversation_id: å¯¹è¯IDï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
            files: æ–‡ä»¶åˆ—è¡¨ï¼ˆUploadFile å¯¹è±¡ï¼‰
            
        Returns:
            ä¸Šä¼ ç»“æœå­—å…¸
        """
        # è‡ªåŠ¨åˆ›å»ºå¯¹è¯ï¼ˆå¦‚æœ conversation_id ä¸º None æˆ– "new"ï¼‰
        if conversation_id is None or conversation_id == "new":
            # åŸºäºç¬¬ä¸€ä¸ªæ–‡ä»¶åç”Ÿæˆå¯¹è¯æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œå¦‚ä¸æä¾›åˆ™ä½¿ç”¨è‡ªåŠ¨ç¼–å·ï¼‰
            first_filename = files[0].filename if files else "æ–°å¯¹è¯"
            title = Path(first_filename).stem if files else None
            conversation_id = self.conversation_service.create_conversation(title=title)
        
        # éªŒè¯å¯¹è¯æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
        conversation = self.conversation_service.get_conversation(conversation_id)
        if not conversation:
            # å¯¹è¯ä¸å­˜åœ¨æ—¶è‡ªåŠ¨åˆ›å»º
            conversation_id = self.conversation_service.create_conversation(title=None)
            conversation = self.conversation_service.get_conversation(conversation_id)
        
        # æ£€æŸ¥å½“å‰æ–‡ä»¶æ•°é‡
        status = self._load_status(conversation_id)
        current_file_count = len(status.get("documents", {}))
        
        if current_file_count + len(files) > config.settings.max_files_per_conversation:
            raise ValueError(
                f"å¯¹è¯å·²æœ‰ {current_file_count} ä¸ªæ–‡ä»¶ï¼Œå†ä¸Šä¼  {len(files)} ä¸ªå°†è¶…è¿‡é™åˆ¶ "
                f"({config.settings.max_files_per_conversation} ä¸ª)"
            )
        
        uploaded_files = []
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            is_valid, error_msg = self._validate_file(file.filename)
            if not is_valid:
                raise ValueError(error_msg)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = await file.read()
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            is_valid, error_msg = await self._check_file_size(file_content)
            if not is_valid:
                raise ValueError(error_msg)
            
            # ä¿å­˜æ–‡ä»¶
            file_info = self.file_manager.save_file(
                conversation_id=conversation_id,
                file_content=file_content,
                original_filename=file.filename
            )
            
            # åˆ›å»ºæ–‡æ¡£è®°å½•
            document_id = file_info["file_id"]
            now = datetime.utcnow().isoformat() + "Z"
            
            document_data = {
                "file_id": document_id,
                "conversation_id": conversation_id,
                "filename": file.filename,
                "file_size": file_info["file_size"],
                "file_extension": file_info["file_extension"],
                "file_path": file_info["file_path"],
                "upload_time": now,
                "status": "pending",
                "lightrag_track_id": None,
            }
            
            # æ›´æ–°çŠ¶æ€
            status = self._load_status(conversation_id)
            if "documents" not in status:
                status["documents"] = {}
            status["documents"][document_id] = document_data
            self._save_status(conversation_id, status)
            
            # æ›´æ–°å¯¹è¯æ–‡ä»¶è®¡æ•°
            self.conversation_service.increment_file_count(conversation_id)
            
            uploaded_files.append({
                "file_id": document_id,
                "filename": file.filename,
                "file_size": file_info["file_size"],
                "status": "pending"
            })
        
        return {
            "conversation_id": conversation_id,
            "uploaded_files": uploaded_files,
            "total_files": len(uploaded_files)
        }
    
    async def build_page_index_json(
        self,
        conversation_id: str,
        document_id: str,
        file_path: str
    ) -> None:
        """
        è§£ææŒ‡å®šæ–‡æ¡£ï¼Œæ„å»ºå¹¶ä¿å­˜ page_index JSON:
        backend/uploads/metadata/page_index/{conversation_id}/{document_id}.json
        """
        try:
            print(f"ğŸ“– å¼€å§‹æ„å»ºé¡µçº§ç´¢å¼•: {document_id[:8]}...")
            
            # ç¡®å®šæ–‡ä»¶ç±»å‹
            file_ext = Path(file_path).suffix.lower()
            pages = []
            
            if file_ext == '.pdf':
                from app.utils.pdf_parser import PDFParser
                parser = PDFParser()
                pages = parser.extract_pages(file_path, file_id=document_id)
            elif file_ext in ['.ppt', '.pptx']:
                from app.utils.ppt_parser import PPTParser
                parser = PPTParser()
                pages = parser.extract_pages(file_path, file_id=document_id)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒæ„å»ºé¡µçº§ç´¢å¼•çš„æ–‡ä»¶ç±»å‹: {file_ext}")
                return

            if not pages:
                print(f"âš ï¸ æ–‡æ¡£ {document_id[:8]} æœªæå–åˆ°ä»»ä½•é¡µé¢å†…å®¹")
                return

            # æ„å»º JSON æ•°æ®
            document_info = self.get_document(conversation_id, document_id)
            # ä¼˜å…ˆä½¿ç”¨ document_status ä¸­çš„ filenameï¼ˆçœŸæ­£çš„åŸå§‹æ–‡ä»¶åï¼‰
            # å¦‚æœ document_info ä¸å­˜åœ¨æˆ– filename ä¸ºç©ºï¼Œè¯´æ˜å…ƒæ•°æ®æœ‰é—®é¢˜ï¼Œä¸åº”è¯¥ç”¨ file_path æ¨å¯¼
            if document_info and document_info.get("filename"):
                filename = document_info["filename"]
            else:
                # å¦‚æœ document_info ä¸å­˜åœ¨æˆ– filename ä¸ºç©ºï¼Œè®°å½•è­¦å‘Šä½†ä½¿ç”¨ file_path ä½œä¸º fallback
                print(f"âš ï¸ è­¦å‘Šï¼šæ–‡æ¡£ {document_id[:8]} çš„ filename åœ¨ document_status ä¸­ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œä½¿ç”¨ file_path ä½œä¸º fallback")
                filename = Path(file_path).name
            
            page_index_data = {
                "conversation_id": conversation_id,
                "document_id": document_id,
                "filename": filename,
                "pages": pages
            }
            
            # ç¡®å®šå­˜å‚¨è·¯å¾„
            # uploads/metadata/page_index/{conversation_id}/{document_id}.json
            page_index_dir = Path(config.settings.conversations_metadata_dir) / "page_index" / conversation_id
            page_index_dir.mkdir(parents=True, exist_ok=True)
            
            page_index_file = page_index_dir / f"{document_id}.json"
            
            # å†™å…¥æ–‡ä»¶
            with open(page_index_file, 'w', encoding='utf-8') as f:
                json.dump(page_index_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… é¡µçº§ç´¢å¼•æ„å»ºå®Œæˆ: {page_index_file}")
            
        except Exception as e:
            print(f"âŒ æ„å»ºé¡µçº§ç´¢å¼•å¤±è´¥: {document_id[:8]}... é”™è¯¯: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹

    async def process_document(self, conversation_id: str, document_id: str):
        """å¤„ç†æ–‡æ¡£ï¼šè§£ææ–‡æœ¬å¹¶æ’å…¥ LightRAGï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
        
        ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ–‡æ¡£åœ¨å¤„ç†ï¼Œ
        é¿å… LightRAG å¹¶å‘å†™å…¥å†²çªã€‚
        
        Args:
            conversation_id: å¯¹è¯ID
            document_id: æ–‡æ¡£ID
        """
        global _processing_queue
        
        # å°†æ–‡æ¡£åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
        async with _queue_lock:
            if conversation_id not in _processing_queue:
                _processing_queue[conversation_id] = []
            if document_id not in _processing_queue[conversation_id]:
                _processing_queue[conversation_id].append(document_id)
            queue_position = _processing_queue[conversation_id].index(document_id) + 1
            print(f"ğŸ“‹ æ–‡æ¡£ {document_id[:8]}... åŠ å…¥é˜Ÿåˆ—ï¼Œä½ç½®: {queue_position}")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼ˆåŒä¸€æ—¶é—´åªå¤„ç†1ä¸ªæ–‡æ¡£ï¼‰
        async with _processing_semaphore:
            print(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£: {document_id[:8]}...")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            status = self._load_status(conversation_id)
            if document_id in status.get("documents", {}):
                status["documents"][document_id]["status"] = "processing"
                self._save_status(conversation_id, status)
            
            try:
                # è·å–æ–‡ä»¶è·¯å¾„
                file_path = self.file_manager.get_file_path(conversation_id, document_id)
                if not file_path or not file_path.exists():
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {document_id}")
                
                # è§£ææ–‡æ¡£ï¼Œæå–æ–‡æœ¬ï¼ˆä¼ å…¥ file_id ä»¥åµŒå…¥å…ƒæ•°æ®æ ‡è®°ï¼‰
                text = self.document_parser.extract_text(str(file_path), file_id=document_id)
                
                if not text or not text.strip():
                    raise ValueError("æ–‡æ¡£è§£æåæ–‡æœ¬å†…å®¹ä¸ºç©º")
                
                # æ¸…ç† base64 å­—ç¬¦ä¸²å¹¶ä¿å­˜
                cleaned_text, base64_map = self._clean_base64_and_save(text, conversation_id)
                
                # æ„å»ºé¡µçº§ä¸‰å…ƒåº“ JSONï¼ˆå¼‚æ­¥ä¸é˜»å¡ä¸»æµç¨‹ï¼Œä½†ä¸ºäº†ç®€å•è¿™é‡Œå…ˆåŒæ­¥è°ƒç”¨ï¼‰
                await self.build_page_index_json(conversation_id, document_id, str(file_path))
                
                # ä¸å†è‡ªåŠ¨ç”Ÿæˆæ€ç»´è„‘å›¾ï¼Œæ”¹ä¸ºé€šè¿‡ Agent æ¨¡å¼æŒ‰éœ€ç”Ÿæˆ
                # 1. ç›´æ¥æ’å…¥åˆ° LightRAGï¼ˆç”ŸæˆçŸ¥è¯†å›¾è°±ï¼Œä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬ï¼‰
                print(f"ğŸ“Š å¼€å§‹ç”ŸæˆçŸ¥è¯†å›¾è°±: {document_id[:8]}...")
                track_id = await self.lightrag_service.insert_document(
                    conversation_id=conversation_id,
                    text=cleaned_text,
                    doc_id=document_id
                )
                
                # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆï¼ˆçŸ¥è¯†å›¾è°±æ˜¯å¼‚æ­¥å¤„ç†çš„ï¼Œä¸éœ€è¦ç­‰å¾…ï¼‰
                status = self._load_status(conversation_id)
                if document_id in status.get("documents", {}):
                    status["documents"][document_id]["status"] = "completed"
                    status["documents"][document_id]["lightrag_track_id"] = track_id
                    self._save_status(conversation_id, status)
                
                # æ„å»º/æ›´æ–° å®ä½“â†’é¡µç æ˜ å°„è¡¨
                try:
                    print(f"ğŸ”„ è§¦å‘å®ä½“é¡µç æ˜ å°„æ›´æ–°: {conversation_id}...")
                    from app.services.graph_service import GraphService
                    graph_service = GraphService()
                    await graph_service.build_entity_page_mapping(conversation_id)
                except Exception as e:
                    print(f"âš ï¸ å®ä½“é¡µç æ˜ å°„æ›´æ–°å¤±è´¥: {e}")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ–‡æ¡£å¤„ç†çŠ¶æ€æ ‡è®°ä¸ºå®Œæˆ
                
                print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {document_id[:8]}...")
            
            except Exception as e:
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                status = self._load_status(conversation_id)
                if document_id in status.get("documents", {}):
                    status["documents"][document_id]["status"] = "failed"
                    status["documents"][document_id]["error"] = str(e)
                    self._save_status(conversation_id, status)
                print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_id[:8]}... é”™è¯¯: {e}")
                raise
            finally:
                # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
                async with _queue_lock:
                    if conversation_id in _processing_queue:
                        if document_id in _processing_queue[conversation_id]:
                            _processing_queue[conversation_id].remove(document_id)
                        if not _processing_queue[conversation_id]:
                            del _processing_queue[conversation_id]
    
    async def _generate_mindmap_async(
        self, 
        conversation_id: str, 
        document_id: str, 
        document_text: str
    ):
        """å¼‚æ­¥ç”Ÿæˆæ€ç»´è„‘å›¾ï¼ˆä¸²è¡Œå¤„ç†ï¼Œæ”¯æŒåˆå¹¶ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
            document_id: æ–‡æ¡£ID
            document_text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
        """
        global _mindmap_semaphore, _mindmap_lock
        
        # è·å–å¯¹è¯æ ‡é¢˜ï¼ˆè¯¾ç¨‹åï¼‰
        conversation = self.conversation_service.get_conversation(conversation_id)
        conversation_title = conversation.get("title", "æœªå‘½åè¯¾ç¨‹") if conversation else "æœªå‘½åè¯¾ç¨‹"
        
        # è·å–æ–‡æ¡£æ–‡ä»¶å
        document_info = self.get_document(conversation_id, document_id)
        document_filename = document_info.get("filename", f"æ–‡æ¡£_{document_id[:8]}") if document_info else f"æ–‡æ¡£_{document_id[:8]}"
        
        # è·å–æˆ–åˆ›å»ºè¯¥å¯¹è¯çš„æ€ç»´è„‘å›¾ä¿¡å·é‡ï¼ˆç¡®ä¿ä¸²è¡Œï¼‰
        async with _mindmap_lock:
            if conversation_id not in _mindmap_semaphore:
                _mindmap_semaphore[conversation_id] = asyncio.Semaphore(1)
            semaphore = _mindmap_semaphore[conversation_id]
        
        # ä½¿ç”¨ä¿¡å·é‡ç¡®ä¿åŒä¸€å¯¹è¯çš„æ€ç»´è„‘å›¾ä¸²è¡Œç”Ÿæˆ
        async with semaphore:
            print(f"ğŸ§  å¼€å§‹ç”Ÿæˆæ€ç»´è„‘å›¾: {document_id[:8]}...")
            try:
                # æµå¼ç”Ÿæˆæ€ç»´è„‘å›¾ï¼ˆè‡ªåŠ¨åˆå¹¶åˆ°å·²æœ‰è„‘å›¾ï¼‰
                async for chunk in self.mindmap_service.generate_mindmap_stream(
                    conversation_id,
                    document_text,
                    conversation_title,
                    document_filename,
                    document_id
                ):
                    # æµå¼è¾“å‡ºå¯ä»¥åœ¨è¿™é‡Œå¤„ç†ï¼ˆå¦‚æœéœ€è¦å®æ—¶æ¨é€ï¼‰
                    pass
                print(f"âœ… æ€ç»´è„‘å›¾ç”Ÿæˆå®Œæˆ: {document_id[:8]}...")
            except Exception as e:
                print(f"âŒ æ€ç»´è„‘å›¾ç”Ÿæˆå¤±è´¥: {document_id[:8]}... é”™è¯¯: {e}")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“æ–‡æ¡£å¤„ç†æµç¨‹
    
    def get_document(self, conversation_id: str, file_id: str) -> Optional[Dict]:
        """è·å–æ–‡æ¡£ä¿¡æ¯
        
        Args:
            conversation_id: å¯¹è¯ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        status = self._load_status(conversation_id)
        return status.get("documents", {}).get(file_id)
    
    def list_documents(self, conversation_id: str) -> List[Dict]:
        """åˆ—å‡ºå¯¹è¯çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        status = self._load_status(conversation_id)
        documents = list(status.get("documents", {}).values())
        # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—
        documents.sort(key=lambda x: x.get("upload_time", ""), reverse=True)
        return documents
    
    async def get_document_status(self, conversation_id: str, file_id: str) -> Optional[Dict]:
        """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€
        
        Args:
            conversation_id: å¯¹è¯ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡æ¡£çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…å«è¿›åº¦ä¿¡æ¯
        """
        document = self.get_document(conversation_id, file_id)
        if document:
            status_info = {
                "file_id": file_id,
                "status": document.get("status"),
                "lightrag_track_id": document.get("lightrag_track_id"),
                "error": document.get("error"),
                "upload_time": document.get("upload_time"),
            }
            
            # å¦‚æœæ–‡æ¡£æ­£åœ¨å¤„ç†ä¸­ï¼Œå°è¯•è·å–è¿›åº¦ä¿¡æ¯
            if document.get("status") == "processing":
                try:
                    progress = await self.lightrag_service.get_processing_progress(doc_id=file_id)
                    if progress:
                        status_info["progress"] = progress
                except Exception:
                    # å¦‚æœè·å–è¿›åº¦å¤±è´¥ï¼Œå¿½ç•¥é”™è¯¯
                    pass
            
            return status_info
        return None
    
    async def delete_document(self, conversation_id: str, file_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£
        
        åˆ é™¤æ“ä½œåŒ…æ‹¬ï¼š
        1. ä»æ–‡ä»¶ç³»ç»Ÿåˆ é™¤æ–‡ä»¶
        2. ä»LightRAGçŸ¥è¯†å›¾è°±ä¸­åˆ é™¤ç›¸å…³æ•°æ®
        3. æ¸…ç†å›¾ç‰‡ç¼“å­˜
        4. ä»çŠ¶æ€ä¸­åˆ é™¤è®°å½•
        5. æ›´æ–°å¯¹è¯æ–‡ä»¶è®¡æ•°
        
        Args:
            conversation_id: å¯¹è¯ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        # è·å–æ–‡æ¡£ä¿¡æ¯ï¼ˆç”¨äºæ¸…ç†ç¼“å­˜å’ŒLightRAGæ•°æ®ï¼‰
        document = self.get_document(conversation_id, file_id)
        
        if not document:
            return False
        
        try:
            # 1. ä»LightRAGä¸­åˆ é™¤æ–‡æ¡£æ•°æ®ï¼ˆå¦‚æœå·²å¤„ç†ï¼‰
            if document.get("status") == "completed" and document.get("lightrag_track_id"):
                try:
                    # è·å–LightRAGå®ä¾‹å¹¶åˆ é™¤æ–‡æ¡£
                    lightrag = await self.lightrag_service.get_lightrag_for_conversation(conversation_id)
                    # LightRAGå¯èƒ½æ²¡æœ‰ç›´æ¥çš„åˆ é™¤æ–¹æ³•ï¼Œè¿™é‡Œå…ˆå°è¯•åˆ é™¤ç›¸å…³çš„chunks
                    # æ³¨æ„ï¼šLightRAGå¯èƒ½æ²¡æœ‰æä¾›åˆ é™¤å•ä¸ªæ–‡æ¡£çš„APIï¼Œéœ€è¦æŸ¥çœ‹æ–‡æ¡£
                    # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºLightRAGçš„è®¾è®¡å¯èƒ½æ˜¯ä¸å¯é€†çš„
                    pass
                except Exception as e:
                    # LightRAGåˆ é™¤å¤±è´¥ä¸å½±å“æ–‡ä»¶åˆ é™¤ï¼Œè®°å½•æ—¥å¿—å³å¯
                    print(f"Warning: ä»LightRAGåˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            
            # 2. æ¸…ç†å›¾ç‰‡ç¼“å­˜
            try:
                from app.utils.image_renderer import ImageRenderer
                import app.config as config
                
                image_renderer = ImageRenderer(
                    cache_dir=config.settings.image_cache_dir,
                    resolution=config.settings.image_resolution,
                    cache_expiry_hours=config.settings.image_cache_expiry_hours
                )
                
                file_ext = document.get("file_extension", "")
                if file_ext:
                    # åˆ é™¤è¯¥æ–‡ä»¶çš„æ‰€æœ‰ç¼“å­˜å›¾ç‰‡ï¼ˆåŒ…æ‹¬ç¼©ç•¥å›¾ï¼‰
                    cache_dir = Path(config.settings.image_cache_dir) / file_ext / file_id
                    if cache_dir.exists():
                        import shutil
                        shutil.rmtree(cache_dir, ignore_errors=True)
            except Exception as e:
                # ç¼“å­˜æ¸…ç†å¤±è´¥ä¸å½±å“æ–‡ä»¶åˆ é™¤
                print(f"Warning: æ¸…ç†å›¾ç‰‡ç¼“å­˜å¤±è´¥: {e}")
            
            # 3. åˆ é™¤æ–‡ä»¶
            file_deleted = self.file_manager.delete_file(conversation_id, file_id)
            
            # 4. ä»çŠ¶æ€ä¸­åˆ é™¤
            if file_id in self._load_status(conversation_id).get("documents", {}):
                status = self._load_status(conversation_id)
                del status["documents"][file_id]
                self._save_status(conversation_id, status)
            
            # 5. æ›´æ–°å¯¹è¯æ–‡ä»¶è®¡æ•°
            self.conversation_service.decrement_file_count(conversation_id)
            
            return file_deleted
            
        except Exception as e:
            print(f"Error deleting document {file_id}: {e}")
            # å³ä½¿éƒ¨åˆ†æ“ä½œå¤±è´¥ï¼Œä¹Ÿå°è¯•åˆ é™¤æ–‡ä»¶
            return self.file_manager.delete_file(conversation_id, file_id)
    
    # ========== æŒ‰ subjectId æ“ä½œæ–‡æ¡£çš„æ–¹æ³• ==========
    
    async def upload_documents_for_subject(self, subject_id: str, files: List[UploadFile]) -> Dict:
        """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆæŒ‰ subjectId å­˜å‚¨ï¼‰
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            files: æ–‡ä»¶åˆ—è¡¨ï¼ˆUploadFile å¯¹è±¡ï¼‰
            
        Returns:
            ä¸Šä¼ ç»“æœå­—å…¸
        """
        # æ£€æŸ¥å½“å‰æ–‡ä»¶æ•°é‡
        status = self._load_subject_status(subject_id)
        current_file_count = len(status.get("documents", {}))
        
        if current_file_count + len(files) > config.settings.max_files_per_conversation:
            raise ValueError(
                f"çŸ¥è¯†åº“å·²æœ‰ {current_file_count} ä¸ªæ–‡ä»¶ï¼Œå†ä¸Šä¼  {len(files)} ä¸ªå°†è¶…è¿‡é™åˆ¶ "
                f"({config.settings.max_files_per_conversation} ä¸ª)"
            )
        
        uploaded_files = []
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            is_valid, error_msg = self._validate_file(file.filename)
            if not is_valid:
                raise ValueError(error_msg)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = await file.read()
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            is_valid, error_msg = await self._check_file_size(file_content)
            if not is_valid:
                raise ValueError(error_msg)
            
            # ä¿å­˜æ–‡ä»¶ï¼ˆä½¿ç”¨ subjectIdï¼‰
            file_info = self.file_manager.save_file_for_subject(
                subject_id=subject_id,
                file_content=file_content,
                original_filename=file.filename
            )
            
            # åˆ›å»ºæ–‡æ¡£è®°å½•
            document_id = file_info["file_id"]
            now = datetime.utcnow().isoformat() + "Z"
            
            document_data = {
                "file_id": document_id,
                "subject_id": subject_id,
                "filename": file.filename,
                "file_size": file_info["file_size"],
                "file_extension": file_info["file_extension"],
                "file_path": file_info["file_path"],
                "upload_time": now,
                "status": "pending",
                "lightrag_track_id": None,
            }
            
            # æ›´æ–°çŠ¶æ€
            status = self._load_subject_status(subject_id)
            if "documents" not in status:
                status["documents"] = {}
            status["documents"][document_id] = document_data
            self._save_subject_status(subject_id, status)
            
            uploaded_files.append({
                "file_id": document_id,
                "filename": file.filename,
                "file_size": file_info["file_size"],
                "status": "pending"
            })
        
        return {
            "subject_id": subject_id,
            "uploaded_files": uploaded_files,
            "total_files": len(uploaded_files)
        }
    
    def list_documents_for_subject(self, subject_id: str) -> List[Dict]:
        """åˆ—å‡ºçŸ¥è¯†åº“çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        status = self._load_subject_status(subject_id)
        documents = list(status.get("documents", {}).values())
        # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—
        documents.sort(key=lambda x: x.get("upload_time", ""), reverse=True)
        return documents
    
    def get_document_for_subject(self, subject_id: str, file_id: str) -> Optional[Dict]:
        """è·å–çŸ¥è¯†åº“æ–‡æ¡£ä¿¡æ¯
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        status = self._load_subject_status(subject_id)
        return status.get("documents", {}).get(file_id)
    
    async def get_document_status_for_subject(self, subject_id: str, file_id: str) -> Optional[Dict]:
        """è·å–çŸ¥è¯†åº“æ–‡æ¡£å¤„ç†çŠ¶æ€
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ–‡æ¡£çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…å«è¿›åº¦ä¿¡æ¯
        """
        document = self.get_document_for_subject(subject_id, file_id)
        if document:
            status_info = {
                "file_id": file_id,
                "status": document.get("status"),
                "lightrag_track_id": document.get("lightrag_track_id"),
                "error": document.get("error"),
                "upload_time": document.get("upload_time"),
            }
            
            # å¦‚æœæ–‡æ¡£æ­£åœ¨å¤„ç†ä¸­ï¼Œå°è¯•è·å–è¿›åº¦ä¿¡æ¯
            if document.get("status") == "processing":
                try:
                    progress = await self.lightrag_service.get_processing_progress(doc_id=file_id)
                    if progress:
                        status_info["progress"] = progress
                except Exception:
                    # å¦‚æœè·å–è¿›åº¦å¤±è´¥ï¼Œå¿½ç•¥é”™è¯¯
                    pass
            
            return status_info
        return None
    
    async def delete_document_for_subject(self, subject_id: str, file_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“æ–‡æ¡£
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            file_id: æ–‡ä»¶ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        # è·å–æ–‡æ¡£ä¿¡æ¯
        document = self.get_document_for_subject(subject_id, file_id)
        
        if not document:
            return False
        
        try:
            # 1. ä»LightRAGä¸­åˆ é™¤æ–‡æ¡£æ•°æ®ï¼ˆå¦‚æœå·²å¤„ç†ï¼‰
            # æ³¨æ„ï¼šLightRAG å¯èƒ½éœ€è¦ conversation_idï¼Œè¿™é‡Œæš‚æ—¶è·³è¿‡
            # åç»­å¯ä»¥è€ƒè™‘ä¸ºæ¯ä¸ª subject ç»´æŠ¤ä¸€ä¸ª LightRAG å®ä¾‹
            
            # 2. æ¸…ç†å›¾ç‰‡ç¼“å­˜
            try:
                from app.utils.image_renderer import ImageRenderer
                import app.config as config
                
                file_ext = document.get("file_extension", "")
                if file_ext:
                    # åˆ é™¤è¯¥æ–‡ä»¶çš„æ‰€æœ‰ç¼“å­˜å›¾ç‰‡
                    cache_dir = Path(config.settings.image_cache_dir) / file_ext / file_id
                    if cache_dir.exists():
                        import shutil
                        shutil.rmtree(cache_dir, ignore_errors=True)
            except Exception as e:
                print(f"Warning: æ¸…ç†å›¾ç‰‡ç¼“å­˜å¤±è´¥: {e}")
            
            # 3. åˆ é™¤æ–‡ä»¶
            file_deleted = self.file_manager.delete_file_for_subject(subject_id, file_id)
            
            # 4. ä»çŠ¶æ€ä¸­åˆ é™¤
            status = self._load_subject_status(subject_id)
            if file_id in status.get("documents", {}):
                del status["documents"][file_id]
                self._save_subject_status(subject_id, status)
            
            return file_deleted
            
        except Exception as e:
            print(f"Error deleting document {file_id} for subject {subject_id}: {e}")
            # å³ä½¿éƒ¨åˆ†æ“ä½œå¤±è´¥ï¼Œä¹Ÿå°è¯•åˆ é™¤æ–‡ä»¶
            return self.file_manager.delete_file_for_subject(subject_id, file_id)
    
    async def process_document_for_subject(self, subject_id: str, document_id: str):
        """å¤„ç†çŸ¥è¯†åº“æ–‡æ¡£ï¼šè§£ææ–‡æœ¬å¹¶æ’å…¥ LightRAGï¼ˆå¼‚æ­¥åå°ä»»åŠ¡ï¼‰
        
        æ³¨æ„ï¼šLightRAG å½“å‰ä½¿ç”¨ conversation_id ä½œä¸ºå‘½åç©ºé—´ï¼Œ
        è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ subject_id ä½œä¸º LightRAG çš„å‘½åç©ºé—´ï¼ˆå¦‚æœæ”¯æŒçš„è¯ï¼‰
        æˆ–è€…ä¸ºæ¯ä¸ª subject ç»´æŠ¤ä¸€ä¸ªå›ºå®šçš„æ–‡æ¡£å®¹å™¨å¯¹è¯ID
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            document_id: æ–‡æ¡£ID
        """
        # è·å–æ–‡æ¡£ä¿¡æ¯
        document = self.get_document_for_subject(subject_id, document_id)
        if not document:
            print(f"âŒ æ–‡æ¡£ä¸å­˜åœ¨: {document_id}")
            return
        
        file_path = document.get("file_path")
        if not file_path or not Path(file_path).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            status = self._load_subject_status(subject_id)
            if document_id in status.get("documents", {}):
                status["documents"][document_id]["status"] = "failed"
                status["documents"][document_id]["error"] = "æ–‡ä»¶ä¸å­˜åœ¨"
                self._save_subject_status(subject_id, status)
            return
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        async with _processing_semaphore:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            status = self._load_subject_status(subject_id)
            if document_id in status.get("documents", {}):
                status["documents"][document_id]["status"] = "processing"
                self._save_subject_status(subject_id, status)
            
            try:
                # è§£ææ–‡æ¡£ï¼Œæå–æ–‡æœ¬ï¼ˆä¼ å…¥ file_id ä»¥åµŒå…¥å…ƒæ•°æ®æ ‡è®°ï¼‰
                document_text = self.document_parser.extract_text(str(file_path), file_id=document_id)
                
                if not document_text or not document_text.strip():
                    raise ValueError("æ–‡æ¡£è§£æåæ–‡æœ¬å†…å®¹ä¸ºç©º")
                
                # æ¸…ç† base64 å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨ subject_id ä½œä¸ºå‘½åç©ºé—´ï¼‰
                cleaned_text, base64_map = self._clean_base64_and_save_for_subject(
                    document_text, subject_id
                )
                
                # æ„å»ºé¡µçº§ç´¢å¼•
                await self.build_page_index_json_for_subject(
                    subject_id, document_id, file_path
                )
                
                # æ’å…¥åˆ° LightRAGï¼ˆä½¿ç”¨ subject_id ä½œä¸º conversation_idï¼‰
                # LightRAG ä½¿ç”¨ conversation_id ä½œä¸ºå‘½åç©ºé—´ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ subject_id
                print(f"ğŸ“Š å¼€å§‹ç”ŸæˆçŸ¥è¯†å›¾è°±: {document_id[:8]}...")
                track_id = await self.lightrag_service.insert_document(
                    conversation_id=subject_id,  # ä½¿ç”¨ subject_id ä½œä¸º conversation_id
                    text=cleaned_text,
                    doc_id=document_id
                )
                
                # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
                status = self._load_subject_status(subject_id)
                if document_id in status.get("documents", {}):
                    status["documents"][document_id]["status"] = "completed"
                    status["documents"][document_id]["lightrag_track_id"] = track_id
                    self._save_subject_status(subject_id, status)
                
                # æ„å»º/æ›´æ–° å®ä½“â†’é¡µç æ˜ å°„è¡¨
                try:
                    from app.services.graph_service import GraphService
                    graph_service = GraphService()
                    await graph_service.build_entity_page_mapping(subject_id, document_ids=[document_id])
                except Exception as e:
                    print(f"âš ï¸ å®ä½“é¡µç æ˜ å°„æ›´æ–°å¤±è´¥: {e}")
                
                print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {document_id[:8]}...")
            
            except Exception as e:
                # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
                status = self._load_subject_status(subject_id)
                if document_id in status.get("documents", {}):
                    status["documents"][document_id]["status"] = "failed"
                    status["documents"][document_id]["error"] = str(e)
                    self._save_subject_status(subject_id, status)
                print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_id[:8]}... é”™è¯¯: {e}")
                raise
    
    def _clean_base64_and_save_for_subject(self, text: str, subject_id: str) -> Tuple[str, Dict[str, str]]:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„ base64 å­—ç¬¦ä¸²ï¼Œä¿å­˜åˆ° base_64.jsonï¼ˆæŒ‰ subjectIdï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            subject_id: çŸ¥è¯†åº“ID
            
        Returns:
            (æ¸…ç†åçš„æ–‡æœ¬, base64æ˜ å°„å­—å…¸ {åºå·: base64å­—ç¬¦ä¸²})
        """
        # è·å– base64.json æ–‡ä»¶è·¯å¾„
        print("cleaning base64 for subject", "="*70)
        base_working_dir = Path(config.settings.lightrag_working_dir)
        base64_file = base_working_dir.parent / subject_id / subject_id / "base_64.json"
        base64_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½å·²æœ‰çš„ base64 æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        base64_map = {}
        if base64_file.exists():
            try:
                with open(base64_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    base64_map = existing_data if isinstance(existing_data, dict) else {}
            except:
                base64_map = {}
        
        # è·å–ä¸‹ä¸€ä¸ªåºå·
        existing_indices = [int(k) for k in base64_map.keys() if k.isdigit()]
        next_index = max(existing_indices, default=0) + 1
        
        cleaned_text = text
        
        # å¤„ç† <latexit> æ ‡ç­¾
        latexit_pattern = r'<latexit[^>]*>([^<]*)</latexit>'
        
        def replace_latexit(match):
            nonlocal next_index
            full_match = match.group(0)
            tag_content = match.group(1).strip()
            
            sha1_match = re.search(r'sha1_base64="([^"]+)"', full_match)
            base64_in_content = re.search(r'[A-Za-z0-9+/=]{50,}', tag_content)
            
            if base64_in_content:
                base64_value = base64_in_content.group(0)
            elif sha1_match:
                base64_value = sha1_match.group(1)
            elif tag_content and len(tag_content) > 20:
                base64_value = tag_content
            else:
                return ""
            
            index_str = str(next_index)
            base64_map[index_str] = base64_value
            next_index += 1
            return f"[BASE64_{index_str}]"
        
        cleaned_text = re.sub(latexit_pattern, replace_latexit, cleaned_text, flags=re.DOTALL)
        
        # å¤„ç†ç‹¬ç«‹çš„ base64 å­—ç¬¦ä¸²
        standalone_base64_pattern = r'(?<!\[BASE64_)[A-Za-z0-9+/=]{50,}(?!\])'
        
        def replace_standalone(match):
            nonlocal next_index
            base64_str = match.group(0)
            if re.match(r'^[A-Za-z0-9+/=]+$', base64_str):
                index_str = str(next_index)
                base64_map[index_str] = base64_str
                next_index += 1
                return f"[BASE64_{index_str}]"
            return base64_str
        
        cleaned_text = re.sub(standalone_base64_pattern, replace_standalone, cleaned_text)
        
        # ä¿å­˜ base64 æ˜ å°„
        if base64_map:
            with open(base64_file, 'w', encoding='utf-8') as f:
                json.dump(base64_map, f, ensure_ascii=False, indent=2)
        
        return cleaned_text, base64_map
    
    async def build_page_index_json_for_subject(
        self,
        subject_id: str,
        document_id: str,
        file_path: str
    ) -> None:
        """è§£ææŒ‡å®šæ–‡æ¡£ï¼Œæ„å»ºå¹¶ä¿å­˜ page_index JSONï¼ˆæŒ‰ subjectIdï¼‰
        
        Args:
            subject_id: çŸ¥è¯†åº“ID
            document_id: æ–‡æ¡£ID
            file_path: æ–‡ä»¶è·¯å¾„
        """
        try:
            print(f"ğŸ“– å¼€å§‹æ„å»ºé¡µçº§ç´¢å¼•: {document_id[:8]}...")
            
            # ç¡®å®šæ–‡ä»¶ç±»å‹
            file_ext = Path(file_path).suffix.lower()
            pages = []
            
            if file_ext == '.pdf':
                from app.utils.pdf_parser import PDFParser
                parser = PDFParser()
                pages = parser.extract_pages(file_path, file_id=document_id)
            elif file_ext in ['.ppt', '.pptx']:
                from app.utils.ppt_parser import PPTParser
                parser = PPTParser()
                pages = parser.extract_pages(file_path, file_id=document_id)
            else:
                print(f"âš ï¸ ä¸æ”¯æŒæ„å»ºé¡µçº§ç´¢å¼•çš„æ–‡ä»¶ç±»å‹: {file_ext}")
                return

            if not pages:
                print(f"âš ï¸ æ–‡æ¡£ {document_id[:8]} æœªæå–åˆ°ä»»ä½•é¡µé¢å†…å®¹")
                return

            # æ„å»º JSON æ•°æ®
            document_info = self.get_document_for_subject(subject_id, document_id)
            if document_info and document_info.get("filename"):
                filename = document_info["filename"]
            else:
                filename = Path(file_path).name
            
            page_index_data = {
                "subject_id": subject_id,
                "document_id": document_id,
                "filename": filename,
                "pages": pages
            }
            
            # ç¡®å®šå­˜å‚¨è·¯å¾„ï¼šuploads/metadata/subjects/{subject_id}/page_index/{document_id}.json
            page_index_dir = Path(config.settings.conversations_metadata_dir) / "subjects" / subject_id / "page_index"
            page_index_dir.mkdir(parents=True, exist_ok=True)
            
            page_index_file = page_index_dir / f"{document_id}.json"
            
            # å†™å…¥æ–‡ä»¶
            with open(page_index_file, 'w', encoding='utf-8') as f:
                json.dump(page_index_data, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… é¡µçº§ç´¢å¼•æ„å»ºå®Œæˆ: {page_index_file}")
            
        except Exception as e:
            print(f"âŒ æ„å»ºé¡µçº§ç´¢å¼•å¤±è´¥: {document_id[:8]}... é”™è¯¯: {e}")
