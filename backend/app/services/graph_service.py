"""çŸ¥è¯†å›¾è°±æœåŠ¡"""
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
from app.services.lightrag_service import LightRAGService
from app.services.document_service import DocumentService
from app.services.memory_service import MemoryService


class GraphService:
    """çŸ¥è¯†å›¾è°±æœåŠ¡ï¼Œå°è£… LightRAG æŸ¥è¯¢åŠŸèƒ½"""
    
    def __init__(self):
        self.lightrag_service = LightRAGService()
        self.document_service = DocumentService()
        from app.services.conversation_service import ConversationService
        self.conversation_service = ConversationService()
        self.memory_service = MemoryService()
    
    def _parse_file_path_to_doc_info(self, file_path: str, conversation_id: str) -> Optional[Dict[str, Any]]:
        """è§£æ file_path åˆ°æ–‡æ¡£ä¿¡æ¯
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ "uploads/conversations/{conversation_id}/documents/{file_id}.pptx"ï¼‰
            conversation_id: å¯¹è¯ID
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« file_id å’Œ filenameï¼Œå¦‚æœæ— æ³•è§£æè¿”å› None
        """
        if not file_path or file_path == "unknown_source":
            return None
        
        try:
            # ä»è·¯å¾„ä¸­æå– file_id
            # è·¯å¾„æ ¼å¼ï¼šuploads/conversations/{conversation_id}/documents/{file_id}.{ext}
            path_obj = Path(file_path)
            if path_obj.suffix:  # æœ‰æ‰©å±•å
                file_id = path_obj.stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            else:
                return None
            
            # è·å–æ–‡æ¡£ä¿¡æ¯
            status = self.document_service._load_status(conversation_id)
            for doc_id, doc_data in status.get("documents", {}).items():
                if doc_id == file_id:
                    return {
                        "file_id": doc_id,
                        "filename": doc_data.get("filename", path_obj.name),
                        "file_type": path_obj.suffix.lower()
                    }
            
            return None
        except Exception:
            return None
    
    async def _get_source_chunks_info(self, lightrag, source_id: str, conversation_id: str = None) -> List[Dict[str, Any]]:
        """ä» source_id è·å– chunk ä¿¡æ¯ï¼Œç”¨äºæ˜ å°„åˆ°æ–‡æ¡£
        
        Args:
            lightrag: LightRAG å®ä¾‹
            source_id: source_idï¼ˆå¯èƒ½æ˜¯å¤šä¸ª chunk_id ç”¨åˆ†éš”ç¬¦è¿æ¥ï¼‰
            conversation_id: å¯¹è¯IDï¼ˆå¯é€‰ï¼Œç”¨äºè§£ææ–‡æ¡£ä¿¡æ¯ï¼‰
            
        Returns:
            chunk ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å« file_id å’Œ page_indexï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        if not source_id:
            return []
        
        chunks_info = []
        try:
            # source_id å¯èƒ½æ˜¯å¤šä¸ª chunk_id ç”¨ GRAPH_FIELD_SEP åˆ†éš”
            # ä½¿ç”¨æ­£ç¡®çš„åˆ†éš”ç¬¦ï¼šGRAPH_FIELD_SEP = "<SEP>"
            from lightrag.constants import GRAPH_FIELD_SEP
            chunk_ids = source_id.split(GRAPH_FIELD_SEP) if GRAPH_FIELD_SEP in source_id else [source_id]
            
            for chunk_id in chunk_ids:
                if not chunk_id or not chunk_id.startswith("chunk-"):
                    continue
                
                # ä» text_chunks è·å– chunk ä¿¡æ¯
                try:
                    chunk_data = await lightrag.text_chunks.get_by_id(chunk_id)
                    if chunk_data:
                        file_path = chunk_data.get("file_path", "")
                        chunk_info = {
                            "chunk_id": chunk_id,
                            "file_path": file_path,
                            "full_doc_id": chunk_data.get("full_doc_id", ""),
                            "chunk_order_index": chunk_data.get("chunk_order_index", 0),
                        }
                        
                        # å°è¯•ä» chunk_data ä¸­è·å– page_index å’Œ file_id
                        # 1. å…ˆæ£€æŸ¥ chunk_data ä¸­æ˜¯å¦ç›´æ¥å­˜å‚¨äº†
                        page_index = chunk_data.get("page_index") or chunk_data.get("page_number") or chunk_data.get("slide_number")
                        
                        # 2. ä» chunk å†…å®¹ä¸­è§£æé¡µé¢æ ‡è®°å’Œæ–‡ä»¶ID
                        chunk_content = chunk_data.get("content", "")
                        if chunk_content:
                            import re
                        # ä¼˜å…ˆè§£æå®Œæ•´æ ¼å¼ï¼š[FILE:{file_id}][PAGE/SLIDE:{index}]
                        full_match = re.search(r'\[FILE:([^\]]+)\]\[(?:PAGE|SLIDE):(\d+)\]', chunk_content)
                        if full_match:
                            file_id_from_content = full_match.group(1)
                            page_index = int(full_match.group(2))
                            # å¦‚æœæä¾›äº† conversation_idï¼Œç›´æ¥ä½¿ç”¨ä»å†…å®¹ä¸­æå–çš„ file_id
                            if conversation_id:
                                chunk_info["file_id"] = file_id_from_content
                        else:
                            # å…¼å®¹æ—§æ ¼å¼ï¼šåˆ†åˆ«æŸ¥æ‰¾ [FILE:{file_id}]ã€[PAGE:N] æˆ– [SLIDE:N]
                            if page_index is None:
                                page_match = re.search(r'\[PAGE:(\d+)\]', chunk_content)
                                slide_match = re.search(r'\[SLIDE:(\d+)\]', chunk_content)
                            if page_match:
                                page_index = int(page_match.group(1))
                            elif slide_match:
                                page_index = int(slide_match.group(1))
                            
                            # å¦‚æœè¿˜æ²¡æœ‰ file_idï¼Œå°è¯•ä»å†…å®¹ä¸­æå–
                            if conversation_id and not chunk_info.get("file_id"):
                                file_match = re.search(r'\[FILE:([^\]]+)\]', chunk_content)
                                if file_match:
                                    chunk_info["file_id"] = file_match.group(1)
                    
                        if page_index is not None:
                            chunk_info["page_index"] = int(page_index)
                        
                        # å¦‚æœæä¾›äº† conversation_id ä¸”è¿˜æ²¡æœ‰ file_idï¼Œå°è¯•ä» file_path è§£æ
                        if conversation_id and not chunk_info.get("file_id") and file_path and file_path != "unknown_source":
                            doc_info = self._parse_file_path_to_doc_info(file_path, conversation_id)
                            if doc_info:
                                chunk_info["file_id"] = doc_info["file_id"]
                                chunk_info["filename"] = doc_info["filename"]
                        
                        chunks_info.append(chunk_info)
                except Exception:
                    continue
        except Exception:
            pass
        
        return chunks_info
    
    async def get_all_entities(self, conversation_id: str) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯çš„æ‰€æœ‰å®ä½“
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            å®ä½“åˆ—è¡¨
        """
        lightrag = await self.lightrag_service.get_lightrag_for_conversation(conversation_id)
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹ï¼ˆå®ä½“ï¼‰
        entities = await lightrag.chunk_entity_relation_graph.get_all_nodes()
        
        # entities å·²ç»æ˜¯ list[dict] æ ¼å¼
        entity_list = []
        for entity_data in entities:
            # entity_data æ˜¯å­—å…¸ï¼Œid å­—æ®µå°±æ˜¯èŠ‚ç‚¹IDï¼ˆå®ä½“åç§°ï¼‰
            entity_id = entity_data.get("id", "")
            source_id = entity_data.get("source_id", "")
            file_path = entity_data.get("file_path", "")
            
            # è§£ææ¥æºä¿¡æ¯
            source_documents = []
            if source_id:
                chunks_info = await self._get_source_chunks_info(lightrag, source_id, conversation_id)
                # ä» chunks ä¸­æå–å”¯ä¸€çš„æ–‡æ¡£ä¿¡æ¯
                seen_file_ids = set()
                for chunk_info in chunks_info:
                    chunk_file_path = chunk_info.get("file_path", "")
                    if chunk_file_path and chunk_file_path != "unknown_source":
                        doc_info = self._parse_file_path_to_doc_info(chunk_file_path, conversation_id)
                        if doc_info and doc_info["file_id"] not in seen_file_ids:
                            seen_file_ids.add(doc_info["file_id"])
                            source_documents.append(doc_info)
            elif file_path and file_path != "unknown_source":
                # å¦‚æœæ²¡æœ‰ source_idï¼Œå°è¯•ç›´æ¥ä» file_path è§£æ
                doc_info = self._parse_file_path_to_doc_info(file_path, conversation_id)
                if doc_info:
                    source_documents.append(doc_info)
            
            entity_list.append({
                "entity_id": entity_id,
                "name": entity_id,  # èŠ‚ç‚¹IDå°±æ˜¯å®ä½“åç§°
                "type": entity_data.get("entity_type", entity_data.get("type", "")),
                "description": entity_data.get("description", ""),
                "source_id": source_id,
                "file_path": file_path,
                "source_documents": source_documents,  # æ¥æºæ–‡æ¡£åˆ—è¡¨
            })
        
        return entity_list
    
    async def get_all_relations(self, conversation_id: str) -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯çš„æ‰€æœ‰å…³ç³»
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            å…³ç³»åˆ—è¡¨
        """
        lightrag = await self.lightrag_service.get_lightrag_for_conversation(conversation_id)
        
        # è·å–æ‰€æœ‰è¾¹ï¼ˆå…³ç³»ï¼‰
        relations = await lightrag.chunk_entity_relation_graph.get_all_edges()
        
        # relations å·²ç»æ˜¯ list[dict] æ ¼å¼
        relation_list = []
        for relation_data in relations:
            # source å’Œ target å·²ç»åœ¨è¾¹æ•°æ®ä¸­
            relation_list.append({
                "relation_id": f"{relation_data.get('source', '')}->{relation_data.get('target', '')}",
                "source": relation_data.get("source", ""),
                "target": relation_data.get("target", ""),
                "type": relation_data.get("relation_type", relation_data.get("type", "")),
                "description": relation_data.get("description", ""),
            })
        
        return relation_list
    
    async def get_entity_detail(self, conversation_id: str, entity_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å®ä½“è¯¦æƒ…
        
        Args:
            conversation_id: å¯¹è¯ID
            entity_id: å®ä½“ID
            
        Returns:
            å®ä½“è¯¦æƒ…ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        entities = await self.get_all_entities(conversation_id)
        
        for entity in entities:
            if entity["entity_id"] == entity_id:
                return entity
        
        return None
    
    async def get_relation_detail(self, conversation_id: str, source: str, target: str) -> Optional[Dict[str, Any]]:
        """è·å–å…³ç³»è¯¦æƒ…
        
        Args:
            conversation_id: å¯¹è¯ID
            source: æºå®ä½“ID
            target: ç›®æ ‡å®ä½“ID
            
        Returns:
            å…³ç³»è¯¦æƒ…ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        relations = await self.get_all_relations(conversation_id)
        
        for relation in relations:
            if relation["source"] == source and relation["target"] == target:
                return relation
        
        return None
    
    def check_has_documents_fast(self, conversation_id: str) -> bool:
        """å¿«é€Ÿæ£€æŸ¥å¯¹è¯æ˜¯å¦æœ‰æ–‡æ¡£ï¼ˆæ— éœ€åˆå§‹åŒ– LightRAGï¼‰
        
        é€šè¿‡æ£€æŸ¥å¯¹è¯å…ƒæ•°æ®ä¸­çš„ file_count æˆ–æ–‡æ¡£çŠ¶æ€æ–‡ä»¶æ¥åˆ¤æ–­ã€‚
        è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ£€æŸ¥ï¼Œç”¨äºåœ¨æŸ¥è¯¢å‰å¿«é€Ÿåˆ¤æ–­æ˜¯å¦éœ€è¦åˆå§‹åŒ– LightRAGã€‚
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            True è¡¨ç¤ºæœ‰æ–‡æ¡£ï¼ŒFalse è¡¨ç¤ºæ²¡æœ‰æ–‡æ¡£
        """
        try:
            # è·å–å¯¹è¯ä¿¡æ¯ï¼Œæå– subject_id
            conversation = self.conversation_service.get_conversation(conversation_id)
            if not conversation:
                return False
            
            subject_id = conversation.get("subject_id")
            
            # å¦‚æœå¯¹è¯æœ‰ subject_idï¼Œæ£€æŸ¥ subject çš„æ–‡æ¡£ï¼ˆå› ä¸ºæ–‡æ¡£ç°åœ¨åŸºäº subject_id å­˜å‚¨ï¼‰
            if subject_id:
                # æ£€æŸ¥ subject çš„æ–‡æ¡£çŠ¶æ€
                status = self.document_service._load_subject_status(subject_id)
                documents = status.get("documents", {})
                if documents:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„æ–‡æ¡£
                    for doc_id, doc_data in documents.items():
                        doc_status = doc_data.get("status", "")
                        # å¦‚æœæ–‡æ¡£çŠ¶æ€æ˜¯ completed æˆ– processingï¼Œè®¤ä¸ºæœ‰æ–‡æ¡£
                        if doc_status in ["completed", "processing"]:
                            return True
                return False
            
            # å›é€€åˆ°æ—§çš„ conversation_id æ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            # æ–¹å¼1ï¼šæ£€æŸ¥å¯¹è¯å…ƒæ•°æ®ä¸­çš„ file_countï¼ˆæœ€å¿«ï¼‰
            file_count = conversation.get("file_count", 0)
            if file_count > 0:
                return True
            
            # æ–¹å¼2ï¼šæ£€æŸ¥æ–‡æ¡£çŠ¶æ€æ–‡ä»¶ï¼ˆæ›´å‡†ç¡®ï¼Œä½†ç¨æ…¢ï¼‰
            status = self.document_service._load_status(conversation_id)
            documents = status.get("documents", {})
            if documents:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„æ–‡æ¡£
                for doc_id, doc_data in documents.items():
                    doc_status = doc_data.get("status", "")
                    # å¦‚æœæ–‡æ¡£çŠ¶æ€æ˜¯ completed æˆ– processingï¼Œè®¤ä¸ºæœ‰æ–‡æ¡£
                    if doc_status in ["completed", "processing"]:
                        return True
            
            # éƒ½æ²¡æœ‰ï¼Œè¿”å› False
            return False
            
        except Exception:
            # æ£€æŸ¥å‡ºé”™æ—¶ï¼Œä¿å®ˆèµ·è§è¿”å› Trueï¼ˆå‡è®¾æœ‰æ–‡æ¡£ï¼Œè¿›å…¥æ­£å¸¸æµç¨‹ï¼‰
            return True
    
    async def check_knowledge_graph_empty(self, conversation_id: str) -> tuple[bool, Optional[str]]:
        """æ£€æµ‹çŸ¥è¯†å›¾è°±æ˜¯å¦ä¸ºç©º
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            (is_empty, error_message): 
            - is_empty: True è¡¨ç¤ºçŸ¥è¯†å›¾è°±ä¸ºç©ºï¼ˆå®ä½“=0 ä¸” å…³ç³»=0 ä¸” æ–‡æ¡£å—=0ï¼‰
            - error_message: å¦‚æœæ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ï¼›å¦åˆ™ä¸º None
        """
        try:
            lightrag = await self.lightrag_service.get_lightrag_for_conversation(conversation_id)
            
            # æ£€æŸ¥å®ä½“æ•°é‡
            entities = await lightrag.chunk_entity_relation_graph.get_all_nodes()
            entity_count = len(entities) if entities else 0
            
            # æ£€æŸ¥å…³ç³»æ•°é‡
            relations = await lightrag.chunk_entity_relation_graph.get_all_edges()
            relation_count = len(relations) if relations else 0
            
            # æ£€æŸ¥æ–‡æ¡£å—æ•°é‡ï¼ˆé€šè¿‡æ£€æŸ¥ chunks_vdb æˆ– text_chunksï¼‰
            # æ³¨æ„ï¼šchunks_vdb å¯èƒ½æ²¡æœ‰ç›´æ¥çš„ count æ–¹æ³•ï¼Œéœ€è¦å°è¯•å…¶ä»–æ–¹å¼
            chunk_count = 0
            try:
                # å°è¯•é€šè¿‡æŸ¥è¯¢ä¸€ä¸ªç©ºæŸ¥è¯¢æ¥è·å–æ€»æ•°ï¼ˆå¦‚æœ API æ”¯æŒï¼‰
                # æˆ–è€…é€šè¿‡æ£€æŸ¥ text_chunks çš„é”®æ•°é‡
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ£€æŸ¥ï¼šå°è¯•è·å–ä¸€äº› chunks
                # ç”±äº LightRAG çš„å­˜å‚¨ç»“æ„ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼æ£€æŸ¥
                # æš‚æ—¶å…ˆæ£€æŸ¥å®ä½“å’Œå…³ç³»ï¼Œå¦‚æœéƒ½ä¸ºç©ºï¼Œè®¤ä¸ºå¯èƒ½æ²¡æœ‰æ–‡æ¡£å—
                pass
            except Exception:
                # å¦‚æœæ— æ³•æ£€æŸ¥æ–‡æ¡£å—æ•°é‡ï¼Œä»…åŸºäºå®ä½“å’Œå…³ç³»åˆ¤æ–­
                pass
            
            # åˆ¤å®šæ ‡å‡†ï¼šå®ä½“=0 ä¸” å…³ç³»=0 æ‰åˆ¤å®šä¸ºç©º
            # æ³¨æ„ï¼šæ–‡æ¡£å—æ•°é‡æ£€æŸ¥å¯èƒ½è¾ƒå¤æ‚ï¼Œæš‚æ—¶ä¸»è¦ä¾èµ–å®ä½“å’Œå…³ç³»
            is_empty = (entity_count == 0 and relation_count == 0)
            
            return is_empty, None
            
        except Exception as e:
            # æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return False, f"æ£€æµ‹çŸ¥è¯†å›¾è°±çŠ¶æ€æ—¶å‡ºé”™: {str(e)}"
    
    async def check_query_result_empty(self, query_result: Dict[str, Any], kg_empty_before: bool) -> tuple[bool, bool]:
        """æ£€æŸ¥æŸ¥è¯¢ç»“æœæ˜¯å¦ä¸ºç©º
        
        Args:
            query_result: aquery_llm è¿”å›çš„ç»“æœå­—å…¸
            kg_empty_before: æŸ¥è¯¢å‰çŸ¥è¯†å›¾è°±æ˜¯å¦ä¸ºç©º
            
        Returns:
            (is_empty, is_no_content):
            - is_empty: True è¡¨ç¤ºæŸ¥è¯¢ç»“æœä¸ºç©ºï¼ˆå®ä½“=0 ä¸” å…³ç³»=0 ä¸” æ–‡æ¡£å—=0ï¼‰
            - is_no_content: True è¡¨ç¤ºæŸ¥è¯¢æœªåŒ¹é…åˆ°ç›¸å…³å†…å®¹ï¼ˆæŸ¥è¯¢å‰æœ‰çŸ¥è¯†å›¾è°±ï¼Œä½†æŸ¥è¯¢åæ— ç»“æœï¼‰
        """
        try:
            # å¦‚æœæŸ¥è¯¢çŠ¶æ€ä¸ºå¤±è´¥ï¼Œç›´æ¥åˆ¤å®šä¸ºç©º
            if query_result.get("status") == "failure":
                return True, False
            
            data = query_result.get("data", {})
            
            entities = data.get("entities", [])
            relationships = data.get("relationships", [])
            chunks = data.get("chunks", [])
            
            entity_count = len(entities) if entities else 0
            relation_count = len(relationships) if relationships else 0
            chunk_count = len(chunks) if chunks else 0
            
            # å…¨éƒ¨ä¸ºç©ºï¼šåˆ¤å®šä¸ºæŸ¥è¯¢ç»“æœä¸ºç©º
            is_empty = (entity_count == 0 and relation_count == 0 and chunk_count == 0)
            
            # æŸ¥è¯¢æœªåŒ¹é…åˆ°ç›¸å…³å†…å®¹ï¼šæŸ¥è¯¢å‰æœ‰çŸ¥è¯†å›¾è°±ï¼Œä½†æŸ¥è¯¢åæ— ç»“æœ
            is_no_content = (not kg_empty_before and is_empty)
            
            return is_empty, is_no_content
            
        except Exception:
            # è§£æå¤±è´¥ï¼Œå‡è®¾ä¸ä¸ºç©º
            return False, False
    
    async def query(self, conversation_id: str, query: str, mode: str = "mix", use_history: bool = True) -> str:
        """åœ¨å¯¹è¯çš„çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢
        
        Args:
            conversation_id: å¯¹è¯ID
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æŸ¥è¯¢æ¨¡å¼ï¼ˆnaive/local/global/mixï¼‰
            use_history: æ˜¯å¦ä½¿ç”¨å†å²å¯¹è¯
            
        Returns:
            æŸ¥è¯¢ç»“æœï¼ˆæ–‡æœ¬ï¼‰
        """
        # è·å–å¯¹è¯ä¿¡æ¯ï¼Œæå– subject_id
        conversation = self.conversation_service.get_conversation(conversation_id)
        subject_id = conversation.get("subject_id") if conversation else None
        
        # å¦‚æœå¯¹è¯æœ‰ subject_idï¼Œä½¿ç”¨ subject_id æ¥æ£€ç´¢çŸ¥è¯†å›¾è°±ï¼ˆå› ä¸ºæ–‡æ¡£å’ŒçŸ¥è¯†å›¾è°±ç°åœ¨åŸºäº subject_id å­˜å‚¨ï¼‰
        # å¦åˆ™å›é€€åˆ°ä½¿ç”¨ conversation_idï¼ˆå‘åå…¼å®¹ï¼‰
        rag_id = subject_id if subject_id else conversation_id
        
        # è·å–å†å²å¯¹è¯ï¼ˆå‡å°‘åˆ°3è½®ï¼Œå¹¶é™åˆ¶å•æ¡æ¶ˆæ¯é•¿åº¦ï¼‰
        history = []
        if use_history:
            history = self.memory_service.get_recent_history(conversation_id, max_turns=3, max_tokens_per_message=1000)
        
        result = await self.lightrag_service.query(rag_id, query, mode=mode, conversation_history=history)
        
        # å¦‚æœç»“æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(result, str):
            return result
        
        # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(result)

    async def build_entity_page_mapping(
        self,
        target_id: str,
        document_ids: Optional[List[str]] = None
    ) -> None:
        """æ„å»ºå®ä½“åˆ°é¡µç çš„æ˜ å°„è¡¨
        
        æ‰«æè¯¥çŸ¥è¯†åº“ä¸‹çš„æ‰€æœ‰å®ä½“ + æ‰€æœ‰ page_index JSONï¼Œ
        è®¡ç®—å®ä½“æœ€ç›¸å…³çš„ (file_id, page_index)ï¼Œ
        å†™å…¥ entity_page_map/{subject_id}.json
        
        Args:
            target_id: ç›®æ ‡IDï¼ˆå¯èƒ½æ˜¯ conversation_id æˆ– subject_idï¼Œä¼šè‡ªåŠ¨è½¬æ¢ä¸º subject_idï¼‰
            document_ids: æŒ‡å®šçš„æ–‡æ¡£IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†è¯¥çŸ¥è¯†åº“ä¸‹çš„æ‰€æœ‰æ–‡æ¡£ï¼‰
        """
        import json
        import re
        import app.config as config
        
        try:
            # å°è¯•è·å– target_id å¯¹åº”çš„ subject_id
            # å¦‚æœ target_id æœ¬èº«å°±æ˜¯ subject_idï¼ˆç”¨äºæ–‡æ¡£å¤„ç†ï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
            # å¦‚æœæ˜¯ conversation_idï¼Œåˆ™å°è¯•è·å–å…¶ subject_id
            subject_id = target_id
            try:
                conversation = self.conversation_service.get_conversation(target_id)
                if conversation and conversation.get("subject_id"):
                    subject_id = conversation["subject_id"]
            except Exception:
                # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹çš„ target_idï¼ˆå‘åå…¼å®¹ï¼‰
                subject_id = target_id
            
            print(f"ğŸ”„ å¼€å§‹æ„å»ºå®ä½“é¡µç æ˜ å°„: {subject_id}...")
            
            # 1. è·å–æ‰€æœ‰å®ä½“ï¼ˆä½¿ç”¨ subject_id ä½œä¸º LightRAG çš„å‘½åç©ºé—´ï¼‰
            entities = await self.get_all_entities(subject_id)
            if not entities:
                print(f"âš ï¸ çŸ¥è¯†åº“ {subject_id} æ²¡æœ‰å®ä½“ï¼Œè·³è¿‡æ˜ å°„æ„å»º")
                return
                
            # 2. åŠ è½½é¡µçº§ç´¢å¼•ï¼ˆä¼˜å…ˆä½¿ç”¨æ–°è·¯å¾„ï¼šsubjects/{subject_id}/page_indexï¼‰
            page_index_dir = Path(config.settings.conversations_metadata_dir) / "subjects" / subject_id / "page_index"
            if not page_index_dir.exists():
                # å›é€€åˆ°æ—§è·¯å¾„ï¼špage_index/{conversation_id}
                page_index_dir = Path(config.settings.conversations_metadata_dir) / "page_index" / target_id
                if not page_index_dir.exists():
                    print(f"âš ï¸ çŸ¥è¯†åº“ {subject_id} æ²¡æœ‰é¡µçº§ç´¢å¼•ç›®å½•ï¼Œè·³è¿‡æ˜ å°„æ„å»º")
                    return
                
            all_pages = [] # List[Dict] -> {file_id, page_index, content}
            
            # éå†è¯¥å¯¹è¯ä¸‹çš„æ‰€æœ‰æ–‡æ¡£ç´¢å¼•æ–‡ä»¶
            for index_file in page_index_dir.glob("*.json"):
                # å¦‚æœæŒ‡å®šäº† document_idsï¼Œåˆ™åªå¤„ç†æŒ‡å®šçš„æ–‡æ¡£
                doc_id = index_file.stem
                if document_ids and doc_id not in document_ids:
                    continue
                    
                try:
                    with open(index_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        file_id = data.get("document_id")
                        pages = data.get("pages", [])
                        for page in pages:
                            all_pages.append({
                                "file_id": file_id,
                                "page_index": page.get("page_index"),
                                "content": page.get("content", "").lower() # è½¬å°å†™ï¼Œæ–¹ä¾¿åŒ¹é…
                            })
                except Exception as e:
                    print(f"âŒ è¯»å–ç´¢å¼•æ–‡ä»¶å¤±è´¥: {index_file}, é”™è¯¯: {e}")
            
            if not all_pages:
                print(f"âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•é¡µé¢å†…å®¹ï¼Œè·³è¿‡æ˜ å°„æ„å»º")
                return
                
            # 3. å®ä½“åŒ¹é…
            entity_page_map = {
                "subject_id": subject_id,
                "entities": {}
            }
            
            # é¢„å¤„ç†å®ä½“åç§°ï¼Œè½¬å°å†™
            processed_entities = []
            for entity in entities:
                name = entity.get("name", "")
                if name:
                    processed_entities.append({
                        "original_name": name,
                        "lower_name": name.lower(),
                        "type": entity.get("type", "")
                    })
            
            # å¯¹æ¯ä¸ªå®ä½“è¿›è¡Œæœç´¢
            for entity in processed_entities:
                name_lower = entity["lower_name"]
                original_name = entity["original_name"]
                
                candidates = []
                
                for page in all_pages:
                    # ç®€å•åŒ¹é…ï¼šè®¡ç®—å®ä½“åç§°åœ¨é¡µé¢ä¸­å‡ºç°çš„æ¬¡æ•°
                    count = page["content"].count(name_lower)
                    
                    if count > 0:
                        candidates.append({
                            "file_id": page["file_id"],
                            "page_index": page["page_index"],
                            "score": count # ç®€å•ä½¿ç”¨é¢‘æ¬¡ä½œä¸ºåˆ†æ•°
                        })
                
                # å¦‚æœæœ‰åŒ¹é…ç»“æœ
                if candidates:
                    # æŒ‰åˆ†æ•°é™åºæ’åˆ—
                    candidates.sort(key=lambda x: x["score"], reverse=True)
                    # åªä¿ç•™å‰ 5 ä¸ªå€™é€‰é¡¹
                    entity_page_map["entities"][original_name] = candidates[:5]
            
            # 4. ä¿å­˜æ˜ å°„è¡¨ï¼ˆä½¿ç”¨ subject_id ä½œä¸ºæ–‡ä»¶åï¼‰
            map_dir = Path(config.settings.conversations_metadata_dir) / "entity_page_map"
            map_dir.mkdir(parents=True, exist_ok=True)
            map_file = map_dir / f"{subject_id}.json"
            
            with open(map_file, 'w', encoding='utf-8') as f:
                json.dump(entity_page_map, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… å®ä½“é¡µç æ˜ å°„æ„å»ºå®Œæˆ: {map_file}, åŒ…å« {len(entity_page_map['entities'])} ä¸ªå®ä½“çš„æ˜ å°„")
            
        except Exception as e:
            print(f"âŒ æ„å»ºå®ä½“é¡µç æ˜ å°„å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def load_entity_page_mapping(
        self,
        target_id: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """è¯»å–å®ä½“é¡µç æ˜ å°„è¡¨
        
        Args:
            target_id: ç›®æ ‡IDï¼ˆå¯èƒ½æ˜¯ conversation_id æˆ– subject_idï¼Œä¼šè‡ªåŠ¨è½¬æ¢ä¸º subject_idï¼‰
            
        Returns:
            å®ä½“æ˜ å°„å­—å…¸ {entity_name: [candidates...]}ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å› {}
        """
        import json
        import app.config as config
        
        try:
            # å°è¯•è·å– target_id å¯¹åº”çš„ subject_id
            subject_id = target_id
            try:
                conversation = self.conversation_service.get_conversation(target_id)
                if conversation and conversation.get("subject_id"):
                    subject_id = conversation["subject_id"]
            except Exception:
                # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹çš„ target_idï¼ˆå‘åå…¼å®¹ï¼‰
                subject_id = target_id
            
            # ä¼˜å…ˆä½¿ç”¨ subject_id ä½œä¸ºæ–‡ä»¶å
            map_file = Path(config.settings.conversations_metadata_dir) / "entity_page_map" / f"{subject_id}.json"
            
            # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ target_idï¼ˆå‘åå…¼å®¹ï¼‰
            if not map_file.exists() and target_id != subject_id:
                map_file = Path(config.settings.conversations_metadata_dir) / "entity_page_map" / f"{target_id}.json"
            
            if map_file.exists():
                with open(map_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("entities", {})
            return {}
            
        except Exception as e:
            print(f"âŒ åŠ è½½å®ä½“é¡µç æ˜ å°„å¤±è´¥: {e}")
            return {}
