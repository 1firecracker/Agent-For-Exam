import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any, List
import app.config as config

# æ·»åŠ  LightRAG è·¯å¾„åˆ° sys.pathï¼ˆå¿…é¡»åœ¨å¯¼å…¥ä¹‹å‰ï¼‰
LIGHTRAG_BASE_PATH = Path(__file__).parent.parent.parent.parent / "LightRAG"
if str(LIGHTRAG_BASE_PATH) not in sys.path:
    sys.path.insert(0, str(LIGHTRAG_BASE_PATH))

# ç°åœ¨å¯ä»¥å¯¼å…¥ LightRAG
from lightrag import LightRAG
from lightrag.kg.shared_storage import initialize_pipeline_status, get_namespace_data
from lightrag.utils import EmbeddingFunc


class LightRAGService:
    """LightRAG æœåŠ¡å°è£…ï¼Œæ”¯æŒå¯¹è¯éš”ç¦»"""
    
    _instance: Optional['LightRAGService'] = None
    _lightrag_instances: Dict[str, LightRAG] = {}  # conversation_id -> LightRAG å®ä¾‹
    _initialized_instances: Dict[str, bool] = {}  # conversation_id -> æ˜¯å¦å·²åˆå§‹åŒ–
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._lightrag_instances = {}
            cls._instance._initialized_instances = {}
        return cls._instance
    
    def clear_all_instances(self):
        """æ¸…é™¤æ‰€æœ‰å·²ç¼“å­˜çš„ LightRAG å®ä¾‹ï¼ˆé…ç½®æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        self._lightrag_instances.clear()
        self._initialized_instances.clear()
        print("ğŸ”„ [LightRAG] å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„å®ä¾‹ï¼Œä¸‹æ¬¡ä½¿ç”¨æ—¶å°†ä½¿ç”¨æ–°é…ç½®é‡æ–°åˆ›å»º")
    
    def get_chat_llm_func(self):
        """è·å–èŠå¤©åœºæ™¯çš„ LLM å‡½æ•°ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰"""
        return self._get_llm_func(use_chat_config=True)
    
    async def _init_lightrag_for_conversation(self, conversation_id: str) -> LightRAG:
        """ä¸ºæŒ‡å®šå¯¹è¯åˆå§‹åŒ– LightRAG å®ä¾‹
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            LightRAG å®ä¾‹
        """
        # å¦‚æœå·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
        if conversation_id in self._lightrag_instances and self._initialized_instances.get(conversation_id, False):
            return self._lightrag_instances[conversation_id]
        
        # ç›´æ¥ä½¿ç”¨ data/<conversation_id> ä½œä¸ºå·¥ä½œç›®å½•ï¼ŒçŸ¥è¯†å›¾è°±æ–‡ä»¶ç›´æ¥ä¿å­˜åœ¨æ­¤ç›®å½•ä¸‹
        working_dir = Path(config.settings.data_dir) / conversation_id
        working_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½® LLM å‡½æ•°
        llm_func = self._get_llm_func()
        
        # é…ç½® Embedding å‡½æ•°
        embedding_func = self._get_embedding_func()
        
        # åˆ›å»º LightRAG å®ä¾‹ï¼ˆä¸è®¾ç½® workspaceï¼Œé¿å…åˆ›å»ºåµŒå¥—å­ç›®å½•ï¼‰
        lightrag = LightRAG(
            working_dir=str(working_dir),
            llm_model_func=llm_func,
            embedding_func=embedding_func,
            kv_storage=config.settings.lightrag_kv_storage,
            vector_storage=config.settings.lightrag_vector_storage,
            graph_storage=config.settings.lightrag_graph_storage,
            doc_status_storage=config.settings.lightrag_doc_status_storage,
            chunk_token_size=600,
            chunk_overlap_token_size=50,
            default_llm_timeout=config.settings.timeout,
        )
        
        # åˆå§‹åŒ–å­˜å‚¨
        await lightrag.initialize_storages()
        await initialize_pipeline_status()
        
        # ç¼“å­˜å®ä¾‹
        self._lightrag_instances[conversation_id] = lightrag
        self._initialized_instances[conversation_id] = True
        
        return lightrag
    
    def _get_llm_func(self, use_chat_config: bool = False):
        """è·å– LLM å‡½æ•°
        
        Args:
            use_chat_config: å¦‚æœä¸º Trueï¼Œä½¿ç”¨èŠå¤©åœºæ™¯é…ç½®ï¼›å¦åˆ™ä½¿ç”¨çŸ¥è¯†å›¾è°±åœºæ™¯é…ç½®
        
        æ”¯æŒ:
        - openai: OpenAI API æˆ–å…¼å®¹ OpenAI API çš„æœåŠ¡ï¼ˆå¦‚ç¡…åŸºæµåŠ¨ï¼‰
        - ollama: Ollama æœ¬åœ°æ¨¡å‹
        """
        from app.services.config_service import config_service
        
        # æ¸…é™¤é…ç½®ç¼“å­˜ï¼Œç¡®ä¿è¯»å–æœ€æ–°é…ç½®
        config_service._config_cache = None
        
        if use_chat_config:
            # ä½¿ç”¨èŠå¤©åœºæ™¯çš„é…ç½®ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
            scene_config = config_service.get_config("chat")
            binding = scene_config.get("binding", config.settings.chat_llm_binding)
            model = scene_config.get("model", config.settings.chat_llm_model)
            api_key = scene_config.get("api_key", config.settings.chat_llm_binding_api_key)
            host = scene_config.get("host", config.settings.chat_llm_binding_host)
            error_msg = "èŠå¤© LLM API Key æœªé…ç½®"
            print(f"ğŸ”§ [LightRAG] ä½¿ç”¨èŠå¤©é…ç½®: binding={binding}, model={model}, host={host[:50]}...")
        else:
            # ä½¿ç”¨çŸ¥è¯†å›¾è°±åœºæ™¯çš„é…ç½®ï¼ˆç”¨äºæ–‡æ¡£æŠ½å–ï¼‰
            scene_config = config_service.get_config("knowledge_graph")
            binding = scene_config.get("binding", config.settings.kg_llm_binding)
            model = scene_config.get("model", config.settings.kg_llm_model)
            api_key = scene_config.get("api_key", config.settings.kg_llm_binding_api_key)
            host = scene_config.get("host", config.settings.kg_llm_binding_host)
            error_msg = "çŸ¥è¯†å›¾è°± LLM API Key æœªé…ç½®"
            print(f"ğŸ”§ [LightRAG] ä½¿ç”¨çŸ¥è¯†å›¾è°±é…ç½®: binding={binding}, model={model}, host={host[:50]}...")
        
        if binding == "openai" or binding == "siliconflow":
            from lightrag.llm.openai import openai_complete_if_cache
            
            if not api_key:
                raise ValueError(error_msg)

            is_siliconcloud_host = host.startswith("https://api.siliconflow.cn")
            
            # æ”¯æŒ OpenAI å…¼å®¹ APIï¼ˆå¦‚ç¡…åŸºæµåŠ¨ï¼‰
            # ä½¿ç”¨ openai_complete_if_cache å‡½æ•°ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å
            def llm_func(prompt, **kwargs):
                # ç§»é™¤ LightRAG å†…éƒ¨å‚æ•°ï¼ˆè¿™äº›å‚æ•°ä¸åº”è¯¥ä¼ é€’ç»™å®é™…çš„ API è°ƒç”¨ï¼‰
                kwargs.pop('api_base', None)
                kwargs.pop('_priority', None)  # LightRAG å†…éƒ¨ä¼˜å…ˆçº§å‚æ•°
                kwargs.pop('_timeout', None)  # LightRAG å†…éƒ¨è¶…æ—¶å‚æ•°
                kwargs.pop('_queue_timeout', None)  # LightRAG å†…éƒ¨é˜Ÿåˆ—è¶…æ—¶å‚æ•°

                if is_siliconcloud_host:
                    existing_extra_body = kwargs.get("extra_body") or {}
                    if "thinking_budget" not in existing_extra_body:
                        extra_body = dict(existing_extra_body)
                        extra_body["thinking_budget"] = 1024
                        kwargs["extra_body"] = extra_body
                    else:
                        kwargs["extra_body"] = existing_extra_body

                # ä½¿ç”¨é…ç½®çš„æ¨¡å‹åï¼Œä¼ é€’ base_url å’Œ api_key
                return openai_complete_if_cache(
                    model=model,
                    prompt=prompt,
                    api_key=api_key,
                    base_url=host,
                    **kwargs
                )
            return llm_func
        elif binding == "ollama":
            from lightrag.llm.ollama import ollama_model_complete
            
            return lambda prompt, **kwargs: ollama_model_complete(
                prompt,
                model=model,
                host=host,
                timeout=config.settings.timeout,
                **kwargs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM binding: {binding}")
    
    def _get_embedding_func(self) -> EmbeddingFunc:
        """è·å– Embedding å‡½æ•°
        
        æ”¯æŒ:
        - ollama: Ollama æœ¬åœ°æ¨¡å‹
        - openai: OpenAI API æˆ–å…¼å®¹ OpenAI API çš„æœåŠ¡
        - siliconcloud: ç¡…åŸºæµåŠ¨ Embedding API
        """
        if config.settings.embedding_binding == "ollama":
            from lightrag.llm.ollama import ollama_embed
            
            return EmbeddingFunc(
                embedding_dim=config.settings.embedding_dim,
                max_token_size=8192,
                func=lambda texts: ollama_embed(
                    texts,
                    embed_model=config.settings.embedding_model,
                    host=config.settings.embedding_binding_host,
                )
            )
        elif config.settings.embedding_binding == "openai":
            from lightrag.llm.openai import openai_embed
            
            api_key = config.settings.llm_binding_api_key
            if not api_key:
                raise ValueError("LLM_BINDING_API_KEY æœªé…ç½®ï¼ˆEmbedding éœ€è¦ï¼‰")
            
            # æ”¯æŒ OpenAI å…¼å®¹ APIï¼ˆå¦‚ç¡…åŸºæµåŠ¨ï¼‰
            from lightrag.llm.openai import openai_embed
            
            return EmbeddingFunc(
                embedding_dim=config.settings.embedding_dim,
                max_token_size=8192,
                func=lambda texts: openai_embed(
                    texts,
                    api_key=api_key,
                    base_url=config.settings.llm_binding_host,
                )
            )
        elif config.settings.embedding_binding == "siliconcloud":
            from lightrag.llm.siliconcloud import siliconcloud_embedding
            
            api_key = config.settings.llm_binding_api_key
            if not api_key:
                raise ValueError("LLM_BINDING_API_KEY æœªé…ç½®ï¼ˆç¡…åŸºæµåŠ¨éœ€è¦ï¼‰")
            
            # ç¡…åŸºæµåŠ¨ Embedding API åœ°å€
            embedding_base_url = config.settings.llm_binding_host.replace("/v1", "/v1/embeddings")
            if not embedding_base_url.endswith("/v1/embeddings"):
                embedding_base_url = "https://api.siliconflow.cn/v1/embeddings"
            
            return EmbeddingFunc(
                embedding_dim=config.settings.embedding_dim,
                max_token_size=8192,
                func=lambda texts: siliconcloud_embedding(
                    texts,
                    model=config.settings.embedding_model,
                    api_key=api_key,
                    base_url=embedding_base_url,
                )
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ Embedding binding: {config.settings.embedding_binding}")
    
    async def get_lightrag_for_conversation(self, conversation_id: str) -> LightRAG:
        """è·å–æŒ‡å®šå¯¹è¯çš„ LightRAG å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            LightRAG å®ä¾‹
        """
        return await self._init_lightrag_for_conversation(conversation_id)
    
    async def insert_document(self, conversation_id: str, text: str, doc_id: Optional[str] = None) -> str:
        """å¼‚æ­¥æ’å…¥æ–‡æ¡£åˆ°æŒ‡å®šå¯¹è¯
        
        Args:
            conversation_id: å¯¹è¯ID
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            track_id: LightRAG å¤„ç†è·Ÿè¸ªID
        """
        lightrag = await self.get_lightrag_for_conversation(conversation_id)
        # ainsert ä½¿ç”¨ input å‚æ•°ï¼Œdoc_id ä½¿ç”¨ ids å‚æ•°
        if doc_id:
            track_id = await lightrag.ainsert(input=text, ids=doc_id)
        else:
            track_id = await lightrag.ainsert(input=text)
        return track_id
    
    async def insert_file(self, conversation_id: str, file_path: str, doc_id: Optional[str] = None) -> str:
        """å¼‚æ­¥æ’å…¥æ–‡ä»¶åˆ°æŒ‡å®šå¯¹è¯
        
        Args:
            conversation_id: å¯¹è¯ID
            file_path: æ–‡ä»¶è·¯å¾„
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            track_id: LightRAG å¤„ç†è·Ÿè¸ªID
        """
        lightrag = await self.get_lightrag_for_conversation(conversation_id)
        if doc_id:
            track_id = await lightrag.ainsert(file_paths=file_path, ids=doc_id)
        else:
            track_id = await lightrag.ainsert(file_paths=file_path)
        return track_id
    
    async def get_processing_progress(self, doc_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–å¤„ç†è¿›åº¦ä¿¡æ¯
        
        Args:
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™åªè¿”å›è¯¥æ–‡æ¡£çš„è¿›åº¦
            
        Returns:
            è¿›åº¦ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« stage, current, total, percentage ç­‰
        """
        try:
            pipeline_status = await get_namespace_data("pipeline_status", first_init=False)
            if not pipeline_status:
                return None
            
            progress = pipeline_status.get("progress")
            if not progress:
                return None
            
            # å¦‚æœæŒ‡å®šäº† doc_idï¼Œåªè¿”å›åŒ¹é…çš„è¿›åº¦
            if doc_id and progress.get("doc_id") != doc_id:
                return None
            
            return progress
        except Exception as e:
            # å¦‚æœ pipeline_status æœªåˆå§‹åŒ–æˆ–å‡ºé”™ï¼Œè¿”å› None
            return None
    
    async def query(self, conversation_id: str, query: str, mode: str = "mix", conversation_history: Optional[List[Dict[str, str]]] = None) -> Any:
        """åœ¨æŒ‡å®šå¯¹è¯çš„çŸ¥è¯†å›¾è°±ä¸­æŸ¥è¯¢ï¼ˆä½¿ç”¨èŠå¤©åœºæ™¯é…ç½®ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æŸ¥è¯¢æ¨¡å¼ï¼ˆnaive/local/global/mixï¼‰
            conversation_history: å¯¹è¯å†å²ï¼Œæ ¼å¼: [{"role": "user/assistant", "content": "..."}]
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        lightrag = await self.get_lightrag_for_conversation(conversation_id)
        
        # ä¸´æ—¶æ›¿æ¢ LLM å‡½æ•°ä¸ºèŠå¤©é…ç½®ï¼ˆç”¨äºæŸ¥è¯¢ï¼‰
        original_llm_func = lightrag.llm_model_func
        chat_llm_func = self._get_llm_func(use_chat_config=True)
        lightrag.llm_model_func = chat_llm_func
        
        from lightrag import QueryParam
        param = QueryParam(mode=mode)
        if conversation_history:
            param.conversation_history = conversation_history
        
        result = await lightrag.aquery(query, param=param)
        
        # æ¢å¤åŸå§‹çš„ LLM å‡½æ•°ï¼ˆç”¨äºæ–‡æ¡£æŠ½å–ï¼‰
        lightrag.llm_model_func = original_llm_func
        
        return result
