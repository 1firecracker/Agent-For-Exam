# -*- coding: utf-8 -*-
# ===========================================================
# æ–‡ä»¶ï¼šbackend/app/agents/agent_b_knowledge_analysis.py
# åŠŸèƒ½ï¼šAgent B - æŒ‰æ•´é¢˜æ±‡æ€»çŸ¥è¯†ç‚¹ï¼ŒæŸ¥è¯¢çŸ¥è¯†å›¾è°±è·å–ç›¸å…³å®ä½“ä¸å‘¨è¾¹çŸ¥è¯†
# ===========================================================

import asyncio
import json
import os
from typing import Dict, List, Set, Tuple

from app.agents.shared_state import shared_state
from app.agents.database.question_bank_storage import BASE_DATA_DIR, load_question_bank
from app.agents.models.quiz_models import QuestionBank, Question, SubQuestion
from app.services.graph_service import GraphService


def _normalize_text(value: str) -> str:
    return str(value or "").strip()


def _normalize_key(value: str) -> str:
    return _normalize_text(value).lower()


def _ensure_list(items) -> List[str]:
    normalized = []
    if isinstance(items, list):
        for item in items:
            text = _normalize_text(item)
            if text:
                normalized.append(text)
    return normalized


def _collect_all_kps(question: Question) -> List[str]:
    """é€’å½’æ”¶é›†é¢˜ç›®åŠæ‰€æœ‰å­é¢˜çš„çŸ¥è¯†ç‚¹ï¼Œå»é‡åè¿”å›"""
    kps: Set[str] = set()

    # æ·»åŠ ä¸»é¢˜çŸ¥è¯†ç‚¹
    for kp in _ensure_list(question.knowledge_points or []):
        kps.add(kp)

    # é€’å½’æ”¶é›†å­é¢˜çŸ¥è¯†ç‚¹
    def collect_from_subs(subs: List[SubQuestion]):
        if not subs:
            return
        for sub in subs:
            for kp in _ensure_list(sub.knowledge_points or []):
                kps.add(kp)
            if sub.sub_questions:
                collect_from_subs(sub.sub_questions)

    if question.sub_questions:
        collect_from_subs(question.sub_questions)

    return list(kps) if kps else ["é€šç”¨çŸ¥è¯†"]


def _summarize_questions(qb: QuestionBank) -> Dict[str, Dict]:
    """æŒ‰æ•´é¢˜æ±‡æ€»ï¼Œè¿”å› {question_id: {stem, knowledge_points}}"""
    summarized: Dict[str, Dict] = {}

    for idx, question in enumerate(qb.questions, 1):
        qid = _normalize_text(getattr(question, "id", "") or f"Q{idx:03d}")
        summarized[qid] = {
            "stem": _normalize_text(question.stem or ""),
            "knowledge_points": _collect_all_kps(question),
        }

    return summarized


def _build_entity_lookup(entities: List[Dict]) -> Tuple[Dict[str, Dict], List[Dict]]:
    """æ„å»ºå®ä½“åç§°ç´¢å¼•"""
    lookup: Dict[str, Dict] = {}
    for entity in entities:
        name = entity.get("name") or entity.get("entity_id") or ""
        normalized = _normalize_key(name)
        if normalized and normalized not in lookup:
            lookup[normalized] = entity
    return lookup, entities


def _collect_relations_map(relations: List[Dict]) -> Dict[str, List[Dict]]:
    """æ„å»ºå®ä½“åˆ°å…³ç³»çš„æ˜ å°„"""
    adjacency: Dict[str, List[Dict]] = {}
    for relation in relations:
        source = relation.get("source") or relation.get("src_id") or ""
        target = relation.get("target") or relation.get("tgt_id") or ""
        for node in (source, target):
            node_key = _normalize_key(node)
            if node_key:
                adjacency.setdefault(node_key, []).append({
                    "source": source,
                    "target": target,
                    "type": relation.get("type") or relation.get("relation_type") or "",
                    "description": relation.get("description", ""),
                })
    return adjacency


def _find_entity(
    knowledge_point: str,
    lookup: Dict[str, Dict],
    all_entities: List[Dict],
) -> Dict | None:
    """ç²¾ç¡®åŒ¹é… + æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾å®ä½“"""
    normalized = _normalize_key(knowledge_point)
    if normalized in lookup:
        return lookup[normalized]
    # æ¨¡ç³ŠåŒ¹é…ï¼šåŒ…å«å…³ç³»
    for entity in all_entities:
        candidate = _normalize_key(entity.get("name") or entity.get("entity_id") or "")
        if candidate and (normalized in candidate or candidate in normalized):
            return entity
    return None


def _get_neighbor_entities(
    entity_name: str,
    relation_map: Dict[str, List[Dict]],
    entity_lookup: Dict[str, Dict],
    max_neighbors: int = 5,
) -> List[Dict]:
    """è·å–å®ä½“çš„é‚»å±…å®ä½“ä¿¡æ¯"""
    neighbors = []
    visited = set()
    entity_key = _normalize_key(entity_name)
    relations = relation_map.get(entity_key, [])

    for rel in relations[:max_neighbors * 2]:  # å¤šå–ä¸€äº›å†å»é‡
        # æ‰¾åˆ°å…³ç³»çš„å¦ä¸€ç«¯
        source = rel.get("source", "")
        target = rel.get("target", "")
        other = target if _normalize_key(source) == entity_key else source

        other_key = _normalize_key(other)
        if other_key in visited or not other_key:
            continue
        visited.add(other_key)

        # æŸ¥æ‰¾é‚»å±…å®ä½“çš„è¯¦ç»†ä¿¡æ¯
        neighbor_entity = entity_lookup.get(other_key, {})
        neighbors.append({
            "entity": other,
            "description": neighbor_entity.get("description", ""),
            "relation_type": rel.get("type", ""),
            "relation_desc": rel.get("description", ""),
        })

        if len(neighbors) >= max_neighbors:
            break

    return neighbors


def _match_graph_data(
    knowledge_points: List[str],
    lookup: Dict[str, Dict],
    all_entities: List[Dict],
    relation_map: Dict[str, List[Dict]],
    max_relations: int = 5,
    max_neighbors: int = 5,
) -> List[Dict]:
    """æ ¹æ®çŸ¥è¯†ç‚¹åŒ¹é…å›¾è°±å®ä½“ã€å…³ç³»åŠå‘¨è¾¹çŸ¥è¯†"""
    matches = []
    visited = set()

    for kp in knowledge_points:
        entity = _find_entity(kp, lookup, all_entities)
        if not entity:
            continue

        entity_name = entity.get("name") or entity.get("entity_id") or ""
        entity_key = _normalize_key(entity_name)
        if not entity_key or entity_key in visited:
            continue
        visited.add(entity_key)

        # è·å–å…³ç³»
        relations = relation_map.get(entity_key, [])[:max_relations]

        # è·å–é‚»å±…å®ä½“ï¼ˆå‘¨è¾¹çŸ¥è¯†ï¼‰
        neighbors = _get_neighbor_entities(
            entity_name, relation_map, lookup, max_neighbors
        )

        matches.append({
            "knowledge_point": kp,
            "entity": entity_name,
            "description": entity.get("description", ""),
            "relations": relations,
            "neighbors": neighbors,
            "source_documents": entity.get("source_documents", []),
        })

    return matches


def _save_related_knowledge(conversation_id: str, related_payload: Dict[str, Dict]) -> str:
    """ä¿å­˜å…³è”çŸ¥è¯†åˆ° JSON æ–‡ä»¶"""
    target_dir = os.path.join(BASE_DATA_DIR, conversation_id)
    if not os.path.exists(target_dir):
        os.makedirs(target_dir, exist_ok=True)
    file_path = os.path.join(target_dir, "related_knowledge.json")
    with open(file_path, "w", encoding="utf-8") as fp:
        json.dump(related_payload, fp, ensure_ascii=False, indent=2)
    return file_path


async def _collect_related_knowledge(
    conversation_id: str,
    summarized_questions: Dict[str, Dict],
) -> Dict[str, Dict]:
    """æ”¶é›†æ¯é“é¢˜çš„çŸ¥è¯†å›¾è°±ç›¸å…³ä¿¡æ¯"""
    graph_service = GraphService()
    has_docs = graph_service.check_has_documents_fast(conversation_id)

    entities: List[Dict] = []
    relations: List[Dict] = []
    if has_docs:
        entities = await graph_service.get_all_entities(conversation_id)
        relations = await graph_service.get_all_relations(conversation_id)
        print(f"ğŸ“Š çŸ¥è¯†å›¾è°±åŠ è½½å®Œæˆï¼š{len(entities)} ä¸ªå®ä½“ï¼Œ{len(relations)} æ¡å…³ç³»")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°çŸ¥è¯†å›¾è°±æ–‡æ¡£ï¼Œå°†è¿”å›ç©ºåŒ¹é…ç»“æœ")

    entity_lookup, entity_list = _build_entity_lookup(entities)
    relation_map = _collect_relations_map(relations)

    result = {}
    for qid, payload in summarized_questions.items():
        kps = payload.get("knowledge_points", [])
        graph_matches = _match_graph_data(
            kps, entity_lookup, entity_list, relation_map
        )
        result[qid] = {
            "knowledge_points": kps,
            "graph_matches": graph_matches,
        }
        print(f"  âœ“ é¢˜ç›® {qid}: {len(kps)} ä¸ªçŸ¥è¯†ç‚¹ â†’ {len(graph_matches)} ä¸ªåŒ¹é…")

    return result


def run_agent_b(conversation_id: str):
    """
    Agent Bï¼š
    æŒ‰æ•´é¢˜æ±‡æ€»çŸ¥è¯†ç‚¹ â†’ æŸ¥è¯¢çŸ¥è¯†å›¾è°± â†’ ä¿å­˜ related_knowledge.json
    """
    print(f"ğŸ§© [Agent B] å¼€å§‹çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œä¼šè¯ID: {conversation_id}")

    qb: QuestionBank = shared_state.question_bank
    if qb is None or not qb.questions:
        print("âš ï¸ shared_state.question_bank ä¸ºç©ºï¼Œå°è¯•ä»ç£ç›˜åŠ è½½ã€‚")
        qb = load_question_bank(conversation_id)

    if qb is None or not qb.questions:
        print("âŒ æ— å¯å¤„ç†é¢˜åº“ï¼ŒAgent B ç»ˆæ­¢ã€‚")
        return None

    summarized = _summarize_questions(qb)
    print(f"ğŸ“‹ å…± {len(summarized)} é“é¢˜ç›®å¾…å¤„ç†")

    related_map = asyncio.run(
        _collect_related_knowledge(conversation_id, summarized)
    )
    file_path = _save_related_knowledge(conversation_id, related_map)

    shared_state.related_knowledge_path = file_path
    print(f"âœ… Agent B å®Œæˆï¼Œå…³è”çŸ¥è¯†å·²ä¿å­˜ï¼š{file_path}")
    return related_map
